In this annotated version of the example:
The order of generation of members of the output list is based on the order of items in the input.
Racket set comprehensions generate Racket sets instead of lists.
Racket hash table comprehensions generate Racket hash tables (one implementation of the Racket dictionary type).
Racket's comprehensions standard library contains parallel and nested versions of its comprehensions, distinguished by "for" vs "for*" in the name. For example, the vector comprehensions "for/vector" and "for*/vector" create vectors by parallel versus nested iteration over sequences. The following is Racket code for the Haskell list comprehension examples.
In Python we could do as follows:
Like the original NPL use, these are fundamentally database access languages.
This makes the comprehension concept more important, because it is computationally infeasible to retrieve the entire list and operate on it (the initial 'entire list' may be an entire XML database).
In XPath, the expression:
So, in another functional language the above FLWOR statement may be implemented like this:
It also offers an alternative comprehension syntax, reminiscent of SQL:
LINQ provides a capability over typical List Comprehension implementations. When the root object of the comprehension implements the IQueryable interface, rather than just executing the chained methods of the comprehension, the entire sequence of commands are converted into an Abstract Syntax Tree (AST) object, which is passed to the IQueryable object to interpret and execute.
This allows, amongst other things, for the IQueryable to
There is some effort in providing C++ with list-comprehension constructs/syntax similar to the set builder notation.
LEESA provides >> for X-Path's / separator. Interestingly, X-Path's // separator that "skips" intermediate nodes in the tree is implemented in LEESA using what's known as Strategic Programming. In the example below, catalog_, book_, author_, and name_ are instances of catalog, book, author, and name classes, respectively.
Side Effects also publishes a partially limited version called Houdini Apprentice, which is free of charge for non-commercial use.
Houdini covers all the major areas of 3D production, including these:
Houdini's operator-based structure is divided into several main groups:
Complex networks can be grouped into a single meta-operator node which behaves like a class definition, and can be instantiated in other networks like any compiled node. In this way users can create their own sophisticated tools without the need for programming. In this way Houdini can be regarded as a highly interactive visual programming toolkit which makes programming more accessible to artists.
Houdini's set of tools are mostly implemented as operators. This has led to a higher learning curve than other comparable tools. It is one thing to know what all the nodes do – but the key to success with Houdini is understanding how to represent a desired creative outcome as a network of nodes. Successful users are generally familiar with a large repertoire of networks (algorithms) which achieve standard creative outcomes. The overhead involved in acquiring this repertoire of algorithms is offset by the artistic and algorithmic flexibility afforded by access to lower level building blocks with which to configure shot element creation routines. In large productions, the development of a procedural network to solve a specific element creation challenge makes automation trivial. Many studios that use Houdini on large feature effects, and feature animation projects develop libraries of procedures that can be used to automate generation of many of the elements for that film with almost no artist interaction.
Typical bug tracking systems support the concept of the life cycle for a bug which is tracked through the status assigned to the bug. A bug tracking system should allow administrators to configure permissions based on status, move the bug to another status, or delete the bug. The system should also allow administrators to configure the bug statuses and to what extent a bug in a particular status can be moved. Some systems will e-mail interested parties, such as the submitter and assigned programmers, when new records are added or the status changes.
The main benefit of a bug-tracking system is to provide a clear centralized overview of development requests (including both bugs and improvements, the boundary is often fuzzy), and their state. The prioritized list of pending items (often called backlog) provides valuable input when defining the product road map, or maybe just "the next release".
In a corporate environment, a bug-tracking system may be used to generate reports on the productivity of programmers at fixing bugs. However, this may sometimes yield inaccurate results because different bugs may have different levels of severity and complexity. The severity of a bug may not be directly related to the complexity of fixing the bug. There may be different opinions among the managers and architects.
There are different kinds of virtual machines, each with different functions:
Both system virtual machines and process virtual machines date to the 1960s, and continue to be areas of active development.
By the 1990s, cell phones and handheld game systems also employed application specific touchscreen GUIs. Newer automobiles use GUIs in their navigation systems and multimedia centers, or navigation multimedia center combinations.
Unity
Xfce
Enlightenment
Sugar
A GUI uses a combination of technologies and devices to provide a platform that users can interact with, for the tasks of gathering and producing information.
GUIs can be made quite hard when dialogs are buried deep in a system, or moved about to different places during redesigns. Also, icons and dialog boxes are usually harder for users to script.
Typical rounding problems are:
The most common type of rounding is to round to an integer; or, more generally, to an integer multiple of some increment — such as rounding to whole tenths of seconds, hundredths of a dollar, to whole multiples of 1/2 or 1/8 inch, to whole dozens or thousands, etc.
For example, by this rule the value 23.5 gets rounded to 24, but −23.5 gets rounded to −23.
The reason for rounding up at 0.5 is that for positive decimals, only the first figure after the decimal point needs be examined. For example, when looking at 17.5000…, the "5" alone determines that the number should be rounded up, to 18 in this case. This is not true for negative decimals, such as −17.5000…, where all the fractional figures of the value need to be examined to determine if it should round to −17 (if it were −17.5000000) or to −18 (if it were −17.5000001 or lower). The common round away from 0 convention for "5" would not have this problem.
For example, 23.5 gets rounded to 23, and −23.5 gets rounded to −24.
For example, 23.5 gets rounded to 23, and −23.5 gets rounded to −23.
This method also treats positive and negative values symmetrically, and therefore is free of overall bias if the original numbers are positive or negative with equal probability.
For example, 23.5 gets rounded to 24, and −23.5 gets rounded to −24.
It is often used for currency conversions and price roundings (when the amount is first converted into the smallest significant subdivision of the currency, such as cents of a euro) as it is easy to explain by just considering the first fractional digit, independently of supplementary precision digits or sign of the amount (for strict equivalence between the paying and recipient of the amount).
This suppresses the random component of the result, if occurrences of 0.5 fractional parts can be effectively numbered. But it can still introduce a positive or negative bias according to the direction of rounding assigned to the first occurrence, if the total number of occurrences is odd.
Some disciplines or institutions have issued standards or directives for rounding.
A software calculator allows the user to perform simple mathematical operations, like addition, multiplication, exponentiation and trigonometry. Data input is typically manual, and the output is a text label.
Music mathematics software utilizes mathematics to analyze or synthesize musical symbols and patterns.
Low-level mathematical libraries intended for use within other programming languages:
Finite element methods are numerical methods for approximating the solutions of mathematical problems that are usually formulated so as to precisely state an idea of some aspect of physical reality.
There are various numerical solution algorithms that can be classified into two broad categories; direct and iterative solvers. These algorithms are designed to exploit the sparsity of matrices that depend on the choices of variational formulation and discretization strategy.
Our explanation will proceed in two steps, which mirror two essential steps one must take to solve a boundary value problem (BVP) using the FEM.
P1 and P2 are ready to be discretized which leads to a common sub-problem (3). The basic idea is to replace the infinite-dimensional linear problem:
with a finite-dimensional version:
Depending on the author, the word "element" in "finite element method" refers either to the triangles in the domain, the piecewise linear basis function, or both. So for instance, an author interested in curved domains might replace the triangles with curved primitives, and so might describe the elements as being curvilinear. On the other hand, some authors replace "piecewise linear" by "piecewise quadratic" or even "piecewise polynomial". The author might then say "higher order element" instead of "higher degree polynomial". Finite element method is not restricted to triangles (or tetrahedra in 3-d, or higher order simplexes in multidimensional spaces), but can be defined on quadrilateral subdomains (hexahedra, prisms, or pyramids in 3-d, and so on). Higher order shapes (curvilinear elements) can be defined with polynomial and even non-polynomial shapes (e.g. ellipse or circle).
More advanced implementations (adaptive finite element methods) utilize a method to assess the quality of the results (based on error estimation theory) and modify the mesh during the solution aiming to achieve approximate solution within some bounds from the 'exact' solution of the continuum problem. Mesh adaptivity may utilize various techniques, the most popular are:
The primary advantage of this choice of basis is that the inner products
and
and
are both zero.
and
be matrices whose entries are
and
then we may rephrase (4) as
In general, the finite element method is characterized by the following process.
The mixed finite element method is a type of finite element method in which extra independent variables are introduced as nodal variables during the discretization of a partial differential equation problem.
Several research codes implement this technique to various degrees: 1. GetFEM++ 2. xfem++ 3. openxfem++
XFEM has also been implemented in codes like Altair Radioss, ASTER, Morfeo and Abaqus. It is increasingly being adopted by other commercial finite element software, with a few plugins and actual core implementations available (ANSYS, SAMCEF, OOFELIE, etc.).
The S-FEM, Smoothed Finite Element Methods, are a particular class of numerical simulation algorithms for the simulation of physical phenomena. It was developed by combining meshfree methods with the finite element method.
A variety of specializations under the umbrella of the mechanical engineering discipline (such as aeronautical, biomechanical, and automotive industries) commonly use integrated FEM in design and development of their products. Several modern FEM packages include specific components such as thermal, electromagnetic, fluid, and structural working environments. In a structural simulation, FEM helps tremendously in producing stiffness and strength visualizations and also in minimizing weight, materials, and costs.
FEM allows detailed visualization of where structures bend or twist, and indicates the distribution of stresses and displacements. FEM software provides a wide range of simulation options for controlling the complexity of both modeling and analysis of a system. Similarly, the desired level of accuracy required and associated computational time requirements can be managed simultaneously to address most engineering applications. FEM allows entire designs to be constructed, refined, and optimized before the design is manufactured.
The functionality of eric is extensible via a plug-in mechanism. The eric plug-in repository provides various kinds of extensions and is accessible from within the IDE.
Prior to the release of eric version 5.5.0, eric version 4 and eric version 5 coexisted and were maintained simultaneously, while eric 4 was the variant for writing software in Python version 2 and eric version 5 was the variant for writing software in Python version 3.
The following table shows the version history of eric, starting from version 4.0.0. Only major (e.g. 6.0.0) and minor (e.g. 6.1.0) releases are listed; revision releases (e.g. 6.0.1) are omitted.
You may Distribute your Modified Version as Source (either gratis or for a Distributor Fee, and with or without a Compiled form of the Modified Version) [...] provided that you do at least ONE of the following:
[...] (c) allow anyone who receives a copy of the Modified Version to make the Source form of the Modified Version available to others under
(i) the Original License or
GitHub is mostly used for code.
GNU is not in the public domain. Everyone will be permitted to modify and redistribute GNU, but no distributor will be allowed to restrict its further redistribution. That is to say, proprietary modifications will not be allowed. I want to make sure that all versions of GNU remain free.
Copyleft licenses necessarily make creative use of relevant rules and laws. For example, when using copyright law, those who contribute to a work under copyleft usually must gain, defer or assign copyright holder status. By submitting the copyright of their contributions under a copyleft license, they deliberately give up some of the rights that normally follow from copyright, including the right to be the unique distributor of copies of the work.
The strength of the copyleft governing a work is an expression of the extent that the copyleft provisions can be efficiently imposed on all kinds of derived works. "Weak copyleft" refers to licenses where not all derived works inherit the copyleft license; whether a derived work inherits or not often depends on the manner in which it was derived.
"Full" and "partial" copyleft relate to another issue: Full copyleft exists when all parts of a work (except the license itself) may only be modified and distributed under the terms of the work's copyleft license. Partial copyleft exempts some parts of the work from the copyleft provisions, thus permitting distribution of some modifications under terms other than the copyleft license, or in some other way does not impose all the principles of copylefting on the work. For example, the GPL linking exception made for some software packages (see below).
W3Techs Web Technology Surveys estimated in September 2016 that:
There are two general approaches to programming language implementation:
Notice that a compiler does not directly execute the program. Ultimately, in order to execute a program via compilation, it must be translated into a form that can serve as input to an interpreter.
Early distributions included the following:
Most distributions install packages, including the kernel and other core operating system components, in a predetermined configuration. Few now require or even permit configuration adjustments at first install time. This makes installation less daunting, particularly for new users, but is not always acceptable. For specific requirements, much software must be carefully configured to be useful, to work correctly with other software, or to be secure, and local administrators are often obliged to spend time reviewing and reconfiguring assorted software.
Some distributions go to considerable lengths to specifically adjust and customize most or all of the software included in the distribution. Not all do so. Some distributions provide configuration tools to assist in this process.
In broad terms, Linux distributions may be:
The diversity of Linux distributions is due to technical, organizational, and philosophical variation among vendors and users. The permissive licensing of free software means that any user with sufficient knowledge and interest can customize an existing distribution or design one to suit his or her own needs.
Other distributions target specific niches, such as:
Various tools are also available to perform full dual-boot installations from existing platforms without a CD, most notably:
Linux and BSD are increasingly filling the market needs traditionally served by proprietary Unix operating systems, as well as expanding into new markets such as the consumer desktop and mobile and embedded devices. Because of the modular design of the Unix model, sharing components is relatively common; consequently, most or all Unix and Unix-like systems include at least some BSD code, and some systems also include GNU utilities in their distributions.
I think the Linux phenomenon is quite delightful, because it draws so strongly on the basis that Unix provided. Linux seems to be the among the healthiest of the direct Unix derivatives, though there are also the various BSD systems as well as the more official offerings from the workstation and mainframe manufacturers.
We intend that this specification should sufficiently document the Java Virtual Machine to make possible compatible clean-room implementations. Oracle provides tests that verify the proper operation of implementations of the Java Virtual Machine.
The class loader performs three basic activities in this strict order:
In general, there are two types of class loader: bootstrap class loader and user defined class loader.
Every Java virtual machine implementation must have a bootstrap class loader, capable of loading trusted classes. The Java virtual machine specification doesn't specify how a class loader should locate classes.
There is no necessary connection between the Java programming language and Java bytecode. A program written in Java can be compiled directly into the machine language of a real computer and programs written in other languages than Java can be compiled into Java bytecode.
With the continuing improvements in JavaScript execution speed, combined with the increased use of mobile devices whose web browsers do not implement support for plugins, there are efforts to target those users through compilation to JavaScript. It is possible to either compile the source code or JVM bytecode to JavaScript. Compiling the JVM bytecode which is universal across JVM languages allows building upon the existing compiler to bytecode.
The JVM specification gives a lot of leeway to implementors regarding the implementation details. Since Java 1.3, JRE from Oracle contains a JVM called HotSpot. It has been designed to be a high-performance JVM.
To speed-up code execution, HotSpot relies on just-in-time compilation. To speed-up object allocation and garbage collection, HotSpot uses generational heap.
BSD licenses are a family of permissive free software licenses, imposing minimal restrictions on the redistribution of covered software. This is in contrast to copyleft licenses, which have reciprocity share-alike requirements. The original BSD license was used for its namesake, the Berkeley Software Distribution (BSD), a Unix-like operating system. The original version has since been revised and its descendants are more properly termed modified BSD licenses. BSD is both a license and a class of license (generally referred to as BSD-like). The modified BSD license (in wide use today) is very similar to the license originally used for the BSD version of Unix. The BSD license is a simple license that merely requires that all code licensed under the BSD license be licensed under the BSD license if redistributed in source code format. BSD (unlike some other licenses) does not require that source code be distributed at all.


In addition to the original (4-clause) license used for BSD, several derivative licenses have emerged that are also commonly referred to as a "BSD license". Today, the typical BSD license is the 3-clause version, which is revised from the original 4-clause version.
In all BSD licenses as following, <organization> is the organization of the <copyright holder> or just the <copyright holder>, and <year> is the year of the copyright. As published in BSD, <copyright holder> is "Regents of the University of California", and <organization> is "University of California, Berkeley".
Some releases of BSD prior to the adoption of the 4-clause BSD license used a license that is clearly ancestral to the 4-clause BSD license. These releases include 4.3BSD-Tahoe (1988) and Net/1 (1989). Though largely replaced by the 4-clause license, this license can be found in 4.3BSD-Reno, Net/2, and 4.4BSD-Alpha.
The original BSD license contained a clause not found in later licenses, known as the "advertising clause". This clause eventually became controversial, as it required authors of all works deriving from a BSD-licensed work to include an acknowledgment of the original source in all advertising material. This was clause number 3 in the original license text:[4]
This clause was objected to on the grounds that as people changed the license to reflect their name or organization it led to escalating advertising requirements when programs were combined together in a software distribution: every occurrence of the license with a different name required a separate acknowledgment. In arguing against it, Richard Stallman has stated that he counted 75 such acknowledgments in a 1997 version of NetBSD.[5] In addition, the clause presented a legal problem for those wishing to publish BSD-licensed software which relies upon separate programs using the GNU GPL: the advertising clause is incompatible with the GPL, which does not allow the addition of restrictions beyond those it already imposes; because of this, the GPL's publisher, the Free Software Foundation, recommends developers not use the license, though it states there is no reason not to use software already using it.[2]
Today, this original license is now sometimes called "BSD-old" or "4-clause BSD".
The advertising clause was removed from the license text in the official BSD on 000000001999-07-22-000022 July 1999 by William Hoskins, Director of the Office of Technology Licensing for UC Berkeley.[4][7] Other BSD distributions removed the clause, but many similar clauses remain in BSD-derived code from other sources, and unrelated code using a derived license.
While the original license is sometimes referred to as the "BSD-old", the resulting 3-clause version is sometimes referred to by "BSD-new." Other names include "New BSD", "revised BSD", "BSD-3", or "3-clause BSD". This version has been vetted as an Open source license by the OSI as "The BSD License".[3] The Free Software Foundation, which refers to the license as the "Modified BSD License", states that it is compatible with the GNU GPL. The FSF encourages users to be specific when referring to the license by name (i.e. not simply referring to it as "a BSD license" or "BSD-style") to avoid confusion with the original BSD license.[6]
This version allows unlimited redistribution for any purpose as long as its copyright notices and the license's disclaimers of warranty are maintained. The license also contains a clause restricting use of the names of contributors for endorsement of a derived work without specific permission.
An even more simplified version has come into use, primarily known for its usage in FreeBSD.[9] It was in use there as early as April 29, 1999 [10] and likely well before. The primary difference between it and the New BSD (3-clause) License is that it omits the non-endorsement clause. It also adds a further disclaimer about views and opinions expressed in the software. The Free Software Foundation, which refers to the license as the FreeBSD License, states that it is compatible with the GNU GPL. In addition, the FSF encourages users to be specific when referring to the license by name (i.e. not simply referring to it as "a BSD license" or "BSD-style"), as it does with the modified/new BSD license, to avoid confusion with the original BSD license.[8]
Other projects, such as NetBSD, use a similar 2-clause license, but without the additional disclaimer.[11] This version has been vetted as an Open source license by the OSI as the "Simplified BSD License."[3]
The ISC license is functionally equivalent, and endorsed by the OpenBSD project as a license template for new contributions.[12]
The FreeBSD project argues on the advantages of BSD-style licenses for companies and commercial use-cases due to their license compatibility with proprietary licenses and general flexibility. The BSD-style licenses place only "minimal restrictions on future behavior" and aren't "legal time-bombs", unlike copyleft licenses.[13] The BSD License allows proprietary use and allows the software released under the license to be incorporated into proprietary products. Works based on the material may be released under a proprietary license as closed source software, allowing usual commercial usages under.
The 3-clause BSD license has, like most permissive licenses, an excellent license compatibility and is compatible with almost all FOSS licenses (and as well proprietary licenses).[14][15]
Two variants of the license, the New BSD License/Modified BSD License (3-clause),[6] and the Simplified BSD License/FreeBSD License (2-clause)[8] have been verified as GPL-compatible free software licenses by the Free Software Foundation, and have been vetted as open source licenses by the Open Source Initiative.[3] The original, 4-clause BSD license has not been accepted as an open source license and, although the original is considered to be a free software license by the FSF, the FSF does not consider it to be compatible with the GPL due to the advertising clause.[2]
Over the years I’ve become convinced that the BSD license is great for code you don’t care about. I’ll use it myself.
The BSD license family is one of the oldest and broadly used license family in the FOSS ecosystem. Also, many new licenses were derived or inspired by the BSD licenses. Many FOSS software projects use a BSD license, for instance the BSD OS family (FreeBSD etc.), Google's Bionic or Toybox. As of 2015 the BSD 3-clause license ranked in popularity number five according to Black Duck Software[17] and sixth according to GitHub data.[18]
One Laptop per Child (OLPC) is a non-profit initiative established with the goal of transforming education for children around the world; this goal was to be achieved by creating and distributing educational devices for the developing world, and by creating software and content for those devices.
Its primary goal continues to be to transform education, by enabling children in low-income countries to have access to content, media and computer-programming environments. At the time that the program launched, the typical retail price for a laptop was considerably in excess of $1,000 (US), so it was infeasible to achieve this objective without also bringing a low-cost machine to production. This became the OLPC XO Laptop, a low-cost and low-power laptop computer. The project was originally funded by member organizations such as AMD, eBay, Google, Marvell Technology Group, News Corporation, Nortel. Chi Mei Corporation, Red Hat, and Quanta provided in-kind support.
The OLPC project has been the subject of extensive praise and criticism. It was praised for enabling low-cost, low-power machines; for assuring consensus at ministerial level in many countries that computer literacy is a mainstream part of education; for creating interfaces that worked without literacy in any language, and particularly without literacy in English. It has received criticism both specific to its mission, and criticism that is typical of many such systems, such as support, ease-of-use, security, content-filtering and privacy issues. Officials in some countries have criticized the project for its appropriateness in terms of price, cultural emphasis and priority as compared to other basic needs of people in third-world settings.


The OLPC program has its roots in the pedagogy of Seymour Papert, an approach known as constructionism, which espoused providing computers for children at early ages to enable full digital literacy. Papert, along with Nicholas Negroponte, were at the MIT Media Lab from its inception. Papert compared the old practice of putting computers in a computer lab to books chained to the walls in old libraries. Negroponte likened shared computers to shared pencils. However, this pattern seemed to be inevitable, given the then-high prices of computers (over $1,500 apiece for a typical laptop or small desktop by 2004).
In 2005, Negroponte spoke at the World Economic Forum, in Davos. In this talk he urged industry to solve the problem, to enable a $100 laptop, which would enable constructionist learning, would revolutionize education, and would bring the world's knowledge to all children. He brought a mock-up and was described as prowling the halls and corridors of Davos to whip up support.[1] Despite the reported skepticism of Bill Gates and others, Negroponte left Davos with committed interest from AMD, News Corp, and with strong indications of support from many other firms. From the outset, it was clear that Negroponte thought that the key to reducing the cost of the laptop was to reduce the cost of the display. Thus, when, upon return from Davos, he met Mary Lou Jepsen, the display pioneer who was in early 2005 joining the MIT Media Lab faculty, the discussions turned quickly to display innovation to enable a low-cost laptop. Convinced that the project was now possible, Negroponte led the creation of the first corporation for this: the Hundred Dollar Laptop Corp.
At the 2006 World Economic Forum in Davos, Switzerland, the United Nations Development Program (UNDP) announced it would back the laptop. UNDP released a statement saying they would work with OLPC to deliver "technology and resources to targeted schools in the least developed countries".[2]
In the first years of the project, the Association managed development and logistics, and the Foundation managed fundraising such as the Give One Get One campaign ("G1G1").
Intel was a member of the association for a brief period in 2007. It resigned its membership on January 3, 2008, citing disagreements with requests from OLPC's founder, Nicholas Negroponte, for Intel to stop dumping their Classmate PCs.[3][4]
In 2008, Negroponte showed some doubt about the exclusive use of open-source software for the project,[5] and made suggestions supporting a move towards adding Windows XP, which Microsoft was in the process of porting over to the XO hardware.[6] Microsoft's Windows XP, however, is not seen by some as a sustainable operating system.[7] Microsoft announced that they would sell them Windows XP for $3 per XO.[8] It would be offered as an option on XO-1 laptops and possibly be able to dual boot alongside Linux.[9] In response, Walter Bender, who was the former President of Software and Content for the OLPC project, left OLPC[10][11] and founded Sugar Labs to continue development of the open source Sugar software which had been developed within OLPC. No significant deployments elected to purchase Windows licenses.
Charles Kane became the new President and Chief Operating Officer of the OLPC Association on May 2, 2008.[12][13] In late 2008, the NYC Department of Education purchased some XO computers for use by New York schoolchildren.[14]
Advertisements for OLPC began streaming on the video streaming website Hulu and others in 2008. One such ad has John Lennon advertising for OLPC, with an unknown voice actor redubbing over Lennon's voice.[15]
In 2008 OLPC reduced their annual budget from $12 million to $5 million which resulted in a restructuring on January 7, 2009. Development of the Sugar operating environment was moved entirely into the community, the Latin America support organization was spun out and staff reductions, including Jim Gettys, affected approximately 50% of the paid employees. The remaining 32 staff members also saw salary reductions.[16][17] Despite the downsizing, OLPC continued development of the XO-1.5 laptops.
In 2010, OLPC moved its headquarters to Miami. The Miami office currently oversees sales and support for the XO-1.5 laptop and its successors, including the XO Laptop version 4.0 and the OLPC Laptop.
Funding from Marvell, finalized in May 2010, revitalized the foundation and enabled the 1Q 2012 completion of the ARM-based XO-1.75 laptops and initial prototypes of the XO-3 tablets. OLPC is now taking orders for mass production of the XO 4.0, and has shipped over 2.5 million XO Laptops to children around the world.
At the World Summit on the Information Society held by the United Nations in Tunisia from November 16–18, 2005, several African representatives, most notably Marthe Dansokho (a missionary of United Methodist Church), voiced suspicions towards the motives of the OLPC project and claimed that the project was using an overly "U.S. mindset", pointing out that the presented solutions were not applicable to specifically "African problems". Dansokho said the project demonstrated misplaced priorities, stating that African women would not have enough time to research new crops to grow. She added that clean water and schools were more important. Mohammed Diop specifically criticized the project as an attempt to exploit the governments of poor nations by making them pay for hundreds of millions of machines and the need of further investments into internet infrastructure.[18] Others have similarly criticized laptop deployments in very low income countries, regarding them as cost-ineffective when compared to far simpler measures such as deworming and other expenses on basic child health.[19]
Lee Felsenstein, a computer engineer who played a central role in the development of the personal computer, criticized the centralized, top-down design and distribution of the OLPC.[20]
The project originally aimed for a price of 100 US dollars. In May 2006, Negroponte told the Red Hat's annual user summit: "It is a floating price. We are a nonprofit organization. We have a target of $100 by 2008, but probably it will be $135, maybe $140."[21] A BBC news article in April 2010 indicated the price still remains above $200.[22]
As of April 2011[update], the price remains above $209.[23] As of 2013[update], more than 10% of the world population lives on less than US$2 per day.[24] The latter income segment would have to spend more than a quarter of its annual income to be able to afford to purchase a single laptop, while the global average of ICT spending is 3% of income.[25] Empirical studies show that the borderline between ICT as a necessity good and ICT as a luxury good is roughly around the "magical number" of US$10 per person per month, or US$120 per year.[25]
John Wood, founder of Room to Read (a NPO which builds schools and libraries), emphasizes affordability and scalability over high-tech solutions. While in favor of the One Laptop per Child initiative for providing education to children in the developing world at a cheaper rate, he has pointed out that a $2,000 library can serve 400 children, costing just $5 a child to bring access to a wide range of books in the local languages (such as Khmer or Nepali) and English; also, a $10,000 school can serve 400–500 children ($20–$25 a child). According to Wood, these are more appropriate solutions for education in the dense forests of Vietnam or rural Cambodia.[26]
The Scandinavian aid organization FAIR proposed setting up computer labs with recycled second-hand computers as a cheaper initial investment. Negroponte argued against this proposition, stating the expensive running cost of conventional laptops.[27] Computer Aid International doubted the OLPC sales strategy would succeed, citing the "untested" nature of its technology. CAI refurbishes computers and printers and sells them to developing countries for £42 a piece (compare it to £50 a piece for the OLPC laptops).[28]
The OLPC project has also been criticized for allegedly adopting a "one-shot" deployment approach with little or no technical support or teacher training, and for neglecting pilot programs and formal assessment of outcomes in favor of quick deployment. Some authors attribute this unconventional approach to the OLPC promoters' alleged focus on constructivist education and 'digital utopianism'.[19] Mark Warschauer, a Professor of University of California at Irvine and Morgan Ames, at the time of writing, a PhD candidate at Stanford University, have pointed out that the laptop by itself does not completely fill the need of students in underprivileged countries. The "children’s machines", as they have been called, have been deployed to several countries, for example Uruguay, Peru, and in the US, Alabama, but after a relatively short time, their usage has declined considerably, sometimes because of hardware problems or breakage, in some cases, as high as 27% to 59% within the first two years, and sometimes due to a lack of knowledge on the part of the users on how to take full advantage of the machine. However, another factor has recently been acknowledged; a lack of a direct relation to the pedagogy needed in the local context to be truly effective. Uruguay reports that only 21.5% of teachers use the laptop in the classroom on a daily basis, and 25% report using it less than once a week. In Alabama, 80.3% of students say they never or seldom use the computer for class work, and Peru, teachers report that in the first few months, 68.9% use the laptop three times per week, but after two months, only 40% report such usage. Those of a low socio-economic level tend to not be able to effectively use the laptop for educational purposes on their own, but with scaffolding and mentoring from teachers, the machine can become more useful. According to one of the returning OLPC executives, Walter Bender, the approach needs to be more holistic, combining technology with a prolonged community effort, teacher training and local educational efforts and insights.[29]
The organization has been accused of simply giving underprivileged children laptops and "walking away". Some critics claim this "drive-by" implementation model was the official strategy of the project. While the organisation has learning teams dedicated to support and working with teachers, Negroponte has said in response to this criticism that "You actually can" give children a connected laptop and walk away, noting experiences with self-guided learning.[30]
In response to such criticism, OLPC began to focus on providing complete solutions to its programming around the world. A fundamental principle of the OLPC program is sustainability. In each program, OLPC works to transfer knowledge and know-how to local teams in order to build capacity and ensure the sustainability of each program. OLPC focuses on seven key components: 1) Implementation Model Design, 2) Inventory and Logistics, 3) Teacher Training, 4) Technical Support, 5) Monitoring and Evaluation, 6) Community Engagement and 7) Volunteer Program.
Each component concentrates on developing the local capacity for a long-term solution. OLPC's multidisciplinary teams work to identify and analyze the social, economic and educational context of each community to create a program tailored to address the specific needs and desired outcomes of each program. OLPC is an educational program with a strong social transformation impact. OLPC's goal is to support the process of program design. With more than 10 years of experience in the field, OLPC is uniquely positioned as an expert in the integration of technology into the learning environment.
The XO, previously known as the "$100 Laptop" or "Children's Machine", is an inexpensive laptop computer designed to be distributed to children in developing countries around the world,[31] to provide them with access to knowledge, and opportunities to "explore, experiment and express themselves" (constructionist learning).[32] The laptop is manufactured by the Taiwanese computer company Quanta Computer.
The rugged, low-power computers use flash memory instead of a hard drive, run a Fedora-based operating system and use the SugarLabs Sugar user interface.[33] Mobile ad hoc networking based on the 802.11s wireless mesh network protocol allows students to collaborate on activities and to share Internet access from one connection. The wireless networking has much greater range than typical consumer laptops. The XO-1 has also been designed to be lower cost and much longer-lived than typical laptops.
In 2009, OLPC announced an updated XO (dubbed XO-1.5) that takes advantage of the latest component technologies. The XO-1.5 includes a new VIA C7-M processor and a new chipset providing a 3D graphics engine and an HD video decoder. It has 1GB of RAM and built-in storage of 4 GB, with an option for 8 GB. The XO-1.5 uses the same display, and a network wireless interface with half the power dissipation.[34]
Early prototype versions of the hardware were available in June 2009, and they are available for software development and testing available for free through a developer's program.[35]
An XO-1.75 model was developed that used a Marvell ARM processor, targeting a price below $150 and date in 2011.[36]
The XO-2 two sheet design concept was canceled in favor of the one sheet XO-3.
An XO-3 concept resembled a tablet computer and was planned to have the inner workings of the XO 1.75.[37] Price goal is below $100 and date is 2012.[38]
As of May 2010, OLPC was working with Marvell on other unspecified future tablet designs.[39] In October 2010, both OLPC and Marvell signed an agreement granting OLPC $5.6 million to fund development of its XO-3 next generation tablet computer. The tablet was to use an ARM chip from Marvell.[40][41]
At CES 2012, OLPC showcased the XO-3 model, which featured a touchscreen and a modified form of SugarLabs "Sugar".[42] In early December 2012, however, it was announced that the XO-3 would not be seeing actual production, and focus had shifted to the XO-4.[43]
The XO-4 was launched at International CES 2013 in Las Vegas[44] The XO Laptop version 4 is available in two models: XO 4 and XO 4 Touch, with the latter providing multi-touch input on the display. The XO Laptop version 4 uses an ARM processor to provide high performance with low power consumption, while keeping the industrial design of the traditional XO Laptop.
The laptops include an anti-theft system which can, optionally, require each laptop to periodically make contact with a server to renew its cryptographic lease token. If the cryptographic lease expires before the server is contacted, the laptop will be locked until a new token is provided. The contact may be to a country-specific server over a network or to a local, school-level server that has been manually loaded with cryptographic "lease" tokens that enable a laptop to run for days or even months between contacts. Cryptographic lease tokens can be supplied on a USB flash drive for non-networked schools.[45] The mass production laptops are also tivoized, disallowing installation of additional software or replacement of the operating system. Users interested in development need to obtain the unlocking key separately (most developer laptops for Western users already come unlocked). It is claimed that locking prevents unintentional bricking and is part of the anti-theft system.[46]
In 2006, the OLPC project was heavily criticised over Red Hat's non-disclosure agreement (NDA) with Marvell concerning the wireless device in OLPC, especially in light of the OLPC project being positioned as an open-source friendly initiative. An open letter for documentation was inked by Theo de Raadt (a recipient of the 2004 Award for the Advancement of Free Software), and the initiative for open documentation has been supported by Richard Stallman, the President of the Free Software Foundation.[47] De Raadt later clarified that he finds an issue with OLPC having proprietary firmware files that are not allowed to be independently re-distributed (even in the binary form) by third-party operating systems like OpenBSD, as well as receiving no documentation to write the necessary drivers for the operating system.[48][49] De Raadt has pointed out that the OpenBSD project requires no firmware source code, and no low-level documentation to work on firmware, only requiring the binary distribution rights and documentation to interface with the said binary firmware that runs outside of the main CPU, a quite simple request that is generally honoured by many other wireless device vendors like Ralink.[50] Stallman fully agreed with de Raadt's request to open up the documentation,[47][not in citation given] since Stallman is known to hold an even stronger and more idealistic position in regards to the proprietary components, and requires that even the firmware that runs outside of the main CPU must be provided in its source code form, something de Raadt does not require. De Raadt later has had to point out that such more idealistic and less realistic position has instead been misattributed to OpenBSD's more practical approach to make it look unreasonable, and stood on record that OpenBSD's position is much easier to satisfy, yet it nonetheless remained unresolved.[48]
OLPC's dedication to "Free and open source" was questioned with their May 15, 2008, announcement that large-scale purchasers would be offered the choice to add an extra cost, special version of the proprietary Windows XP OS developed by Microsoft alongside the regular, free and open Linux-based operating system with the SugarLabs "Sugar OS" GUI. Microsoft developed a modified version of Windows XP and announced in May 2008 that Windows XP will be available for an additional cost of 10 dollars per laptop.[51] James Utzschneider, from Microsoft, said that initially only one operating system could be chosen.[52][53] OLPC, however, said that future OLPC work would enable XO-1 laptops to dual boot either the free and open Linux/Sugar OS or the proprietary Microsoft Windows XP. Negroponte further said that "OLPC will sell Linux-only and dual-boot, and will not sell Windows-only [XO-1 laptops]". OLPC released the first test firmware enabling XO-1 dual-boot on July 3, 2008.[52][54][55][56][57] This option did not prove popular. As of 2011, a few pilots had received a few thousand total dual-boot machines, and the new ARM-based machines do not support Windows XP. No significant deployment purchased Windows licenses.[58] Negroponte stated that the dispute had "become a distraction" for the project, and that its end goal was enabling children to learn, while constructionism and the open source ethos was more of a means to that end.[12] Charles Kane concurred, stating that anything which detracted from the ultimate goal of widespread distribution and use was counterproductive.[12]
The organization has been criticized for its lack of troubleshooting support. Teachers in Peru are told to handle problems in one of two ways. If the problem is a software issue, they are to flash the computer, and if it is a hardware problem, they are to report it. In the classroom environment this black-boxing approach is being criticized for causing the teachers and students to feel disconnected with, and confused by the laptop, which results, in many cases, in the laptops eventually going unused.[59] Several defects in OLPC XO-1 hardware have emerged in the field, and laptop repair is often neglected by students or their families (who are responsible for maintenance) due to the relatively high cost of some components (such as displays).[19]
On the software side, the Bitfrost security system has been known to deactivate improperly, rendering the laptop unusable until it is unlocked by support technicians with the proper keys. (This is a time-consuming process, and the problem often affects large numbers of laptops at the same time). The Sugar interface has been difficult for teachers to learn, and the mesh networking feature in the OLPC XO-1 was buggy and went mostly unused in the field.[19]
The OLPC XO-1 hardware lacks connectivity to external monitors or projectors, and teachers are not provided with software for remote assessment. As a result, students are unable to present their work to the whole class, and teachers must also assess students' work from the individual laptops. Teachers often find it difficult to use the keyboard and screen, which were designed with student use in mind.[19]
In 2005 and prior to the final design of the XO-1 hardware, OLPC received criticism because of concerns over the environmental and health impacts of hazardous materials found in most computers.[60] The OLPC asserted that it aimed to use as many environmentally friendly materials as it could; that the laptop and all OLPC-supplied accessories would be fully compliant with the EU's Restriction of Hazardous Substances Directive (RoHS); and that the laptop would use an order of magnitude less power than the typical consumer netbooks available as of 2007 thus minimizing the environmental burden of power generation.[61]
The XO-1 delivered (starting in 2007) uses environmental friendly materials, complies with the EU's RoHS and uses between 0.25 and 6.5 watts[62] in operation. According to the Green Electronics Council's Electronic Product Environmental Assessment Tool, whose sole purpose is assessing and measuring the impact laptops have on the environment, the XO is not only non-toxic and fully recyclable, but it lasts longer, costs less, and is more energy efficient. The XO-1 is the first laptop to have been awarded an EPEAT Gold level rating.[63][64]
Other discussions question whether OLPC laptops should be designed to promote anonymity or to facilitate government tracking of stolen laptops. A June 2008 New Scientist article critiqued Bitfrost's P_THEFT security option, which allows each laptop to be configured to transmit an individualized, non-repudiable digital signature to a central server at most once each day to remain functioning.[65]
The laptops are sold to governments,[66] to be distributed through the ministries of education with the goal of distributing "one laptop per child". The laptops are given to students, similar to school uniforms and ultimately remain the property of the child. The operating system and software is localized to the languages of the participating countries.
OLPC now also works directly with program sponsors from the public and private sectors to implement its educational program in entire schools and communities. As a non-profit organization, OLPC does require a source of funding for its program so that the laptops are given to students at no cost to child or to his/her family.
Approximately 500 developer boards (Alpha-1) were distributed in mid-2006; 875 working prototypes (Beta 1) were delivered in late 2006; 2400 Beta-2 machines were distributed at the end of February 2007;[67] full-scale production started November 6, 2007.[68] Around one million units were manufactured in 2008.
OLPC initially stated that no consumer version of the XO laptop was planned.[69] The project, however, later established the laptopgiving.org website to accept direct donations and ran a "Give 1 Get 1" (G1G1) offer starting on November 12, 2007. The offer was initially scheduled to run for only two weeks, but was extended until December 31, 2007 to meet demand. With a donation of $399 (plus US$25 shipping cost) to the OLPC "Give 1 Get 1" program, donors received an XO-1 laptop of their own and OLPC sent another on their behalf to a child in a developing country. Shipments of "Get 1" laptops sent to donors were restricted to addresses within the United States, its territories, and Canada.
Some 83,500 people participated in the program. Delivery of all of the G1G1 laptops was completed by April 19, 2008.[70] Delays were blamed on order fulfillment and shipment issues both within OLPC and with the outside contractors hired to manage those aspects of the G1G1 program.[71]
Between November 17 and December 31, 2008, a second G1G1 program[72] was run through Amazon.com and Amazon.co.uk.[73] This partnership was chosen specifically to solve the distribution issues of the G1G1 2007 program. The price to consumers was the same as in 2007, at US$399.
The program aimed to be available worldwide. Laptops could be delivered in the US, in Canada and in more than 30 European countries, as well as in some Central and South American countries (Colombia, Haiti, Peru, Uruguay, Paraguay), African countries (Ethiopia, Ghana, Nigeria, Madagascar, Rwanda) and Asian countries (Afghanistan, Georgia, Kazakhstan, Mongolia, Nepal).[74] Despite this, the program sold only about 12,500 laptops and generated a mere $2.5 million, a 93 percent decline from the year before.[75]
OLPC no longer advertises direct to consumers, focusing instead on fundraising efforts.[citation needed] In 2011, they launched a new website designed by Pentagram[76] and Upstatement.[77]
As of 2015[update], OLPC reports 'more than 3 million laptops' have been shipped.[78]
In October 2007, Uruguay placed an order for 100,000 laptops, making Uruguay the first country to purchase a full order of laptops. The first real, non-pilot deployment of the OLPC technology happened in Uruguay in December 2007.[79] Since then, 200,000 more laptops have been ordered to cover all public school children between 6 and 12 years old.
President Tabaré Vázquez of Uruguay presented the final laptop at a school in Montevideo on October 13, 2009.[80] Over the last two years 362,000 pupils and 18,000 teachers have been involved, and has cost the state $260 (£159) per child, including maintenance costs, equipment repairs, training for the teachers and internet connection.[81] The annual cost of maintaining the programme, including an information portal for pupils and teachers, will be US$21 (£13) per child.[81]
The country reportedly became the first in the world where every primary school child received a free laptop on October 13, 2009 as part of the Plan Ceibal (Education Connect).[81][82] However, the South Pacific island nation of Niue also claimed this in August 2008.[83]
Unfortunately, even though roughly 35% of all OLPC computers went to Uruguay, a 2013 study by the Economics Institute (University of the Republic, Uruguay) of the Ceibal plan concluded that use of the laptops did not improve literacy and that the use of the laptops was mostly recreational, with only 4.1% of the laptops being used "all" or "most" days in 2012. The main conclusion was that the results showed no impact of the OLPC program on the test scores in reading and math.[84]
Originally, OLPC announced the United States would not be part of the first-year effort. In 2008, Nicholas Negroponte said "OLPC America already has a director and a chairman and will likely be based in Washington, D.C.,"[85] however such an organization was not set up. As of 2010, Birmingham, Alabama is the largest deployment in the US. Some said the changing economic landscape forced OLPC to adjust their distribution strategy. Negroponte cited patriotism, "building critical mass", and providing a means for children all over the world to communicate.
On January 26, 2012, prime minister Ara Harutyunyan and entrepreneur Eduardo Eurnekian signed a memorandum of understanding launching an OLPC program in Karabagh. The program is geared towards elementary schools throughout Karabagh. Eurnekian hopes to decrease the gap by giving the war-zoned region an opportunity to engage in a more solid education. The New York-based nonprofit, Armenian General Benevolent Union, is helping to undertake the responsibility by providing on-the-ground support. The government of Karabagh is enthusiastic and is working with OLPC to bring the program to fruition.[86]
Lagos Analysis Corp., also called Lancor, a Lagos, US-based Nigerian-owned company, sued OLPC in the end of 2007 for $20 million, claiming that the computer's keyboard design was stolen from a Lancor patented device.[87] OLPC responded by claiming that they had not sold any multi-lingual keyboards in the design claimed by Lancor,[88] and that Lancor had misrepresented and concealed material facts before the court.[89] In January 2008, the Nigerian Federal Court rejected OLPC motion to dismiss LANCOR's lawsuit and extended its injunction against OLPC distributing its XO Laptops in Nigeria. OLPC appealed the Court's decision, the Appeal is still pending in the Nigerian Federal Court of Appeals. In March 2008, OLPC filed a lawsuit in Massachusetts to stop LANCOR from suing it in the United States.[90] In October 2008, MIT News magazine erroneously reported that the Middlesex Superior Court granted OLPC’s motions to dismiss all of LANCOR’s claims against OLPC, Nicholas Negroponte, and Quanta.[91] On October 22, 2010 OLPC voluntarily moved the Massachusetts Court to dismiss its own lawsuit against LANCOR.
In 2007, XO laptops in Nigeria were reported to contain pornographic material belonging to children participating in the OLPC Program.[92] In response, OLPC Nigeria announced they would start equipping the machines with filters.[92][93]
India's Ministry of Human Resource Development, in June 2006, rejected the initiative, saying "it would be impossible to justify an expenditure of this scale on a debatable scheme when public funds continue to be in inadequate supply for well-established needs listed in different policy documents".[94][95] Later they stated plans to make laptops at $10 each for schoolchildren. Two designs submitted to the Ministry from a final year engineering student of Vellore Institute of Technology and a researcher from the Indian Institute of Science, Bangalore in May 2007 reportedly describe a laptop that could be produced for "$47 per laptop" for even small volumes.[96] The Ministry announced in July 2008 that the cost of their proposed "$10 laptop" would in fact be $100 by the time the laptop became available.[97] In 2010, a related $35 Sakshat Tablet was unveiled in India, released the next year as the "Aakash".[98][99] In 2011, each Aakash sold for approximately $44 by an Indian company, DataWind. DataWind plans to launch similar projects in Brazil, Egypt, Panama, Thailand and Turkey.[100] OLPC later expressed support for the initiative.[101]
In 2009, a number of states announced plans to order OLPCs. However, as of 2010, only the state of Manipur had deployed 1000 laptops.
Help is any form of assisting others.
Help may also refer to:
Bytecode, also termed portable code or p-code, is a form of instruction set designed for efficient execution by a software interpreter. Unlike human-readable source code, bytecodes are compact numeric codes, constants, and references (normally numeric addresses) that encode the result of compiler parsing and semantic analysis of things like type, scope, and nesting depths of program objects. They thus allow much better performance than interpreting source code directly.
The name bytecode stems from instruction sets that have one-byte opcodes followed by optional parameters. Intermediate representations such as bytecode may be output by programming language implementations to ease interpretation, or it may be used to reduce hardware and operating system dependence by allowing the same code to run cross-platform, on different devices. Bytecode may often be either directly executed on a virtual machine (a p-code machine i.e., interpreter), or it may be further compiled into machine code for better performance.
Since bytecode instructions are processed by software, they may be arbitrarily complex, but are nonetheless often akin to traditional hardware instructions: virtual stack machines are the most common, but virtual register machines have been built also.[1][2] Different parts may often be stored in separate files, similar to object modules, but dynamically loaded during execution.
A bytecode program may be executed by parsing and directly executing the instructions, one at a time. This kind of bytecode interpreter is very portable. Some systems, called dynamic translators, or just-in-time (JIT) compilers, translate bytecode into machine code as necessary at runtime. This makes the virtual machine hardware-specific, but doesn't lose the portability of the bytecode. For example, Java and Smalltalk code is typically stored in bytecoded format, which is typically then JIT compiled to translate the bytecode to machine code before execution. This introduces a delay before a program is run, when bytecode is compiled to native machine code, but improves execution speed considerably compared to interpreting source code directly, normally by several orders of magnitude.[citation needed]
Because of its performance advantage, today many language implementations execute a program in two phases, first compiling the source code into bytecode, and then passing the bytecode to the virtual machine. There are bytecode based virtual machines of this sort for Java, Python, PHP,[3] Tcl, mawk and Forth (however, Forth is seldom compiled via bytecodes in this way, and its virtual machine is more generic instead). The implementation of Perl and Ruby 1.8 instead work by walking an abstract syntax tree representation derived from the source code.
More recently, the authors of V8[4] and Dart[5] have challenged the notion that intermediate bytecode is needed for fast and efficient VM implementation. Both of these language implementations currently do direct JIT compiling from source code to machine code with no bytecode intermediary.[6]
This is a list of notable programming languages, grouped by type.
(Because there is no overarching classification scheme for programming languages, in many cases a language will be listed under multiple headings.)


Array programming (also known as vector or multidimensional) languages generalize operations on scalars to apply transparently to vectors, matrices, and higher-dimensional arrays.
Assembly languages directly correspond to a machine language (see below) so machine code instructions appear in a form understandable by humans. Assembly languages lets programmers use symbolic addresses, which the assembler converts to absolute addresses. Most assemblers also support macros and symbolic constants.
Command-line interface (CLI) languages are also called batch languages, or job control languages. Examples:
These are languages typically processed by compilers, though theoretically any language can be compiled or interpreted[citation needed]. See also compiled language.
Message passing languages provide language constructs for concurrency. The predominant paradigm for concurrency in mainstream languages such as Java is shared memory concurrency based on monitors. Concurrent languages that make use of message passing have generally been inspired by CSP or the π-calculus, but have had little commercial success, except for Ada and Erlang. Ada is a multipurpose language and concurrent programming is only one option available.
The curly-bracket or curly-brace programming languages have a syntax that defines statement blocks using the curly bracket or brace characters { and }. This syntax originated with BCPL (1966), and was popularized by C. Many curly-bracket languages descend from or are strongly influenced by C. Examples of curly-bracket languages include:
Dataflow programming languages rely on a (usually visual) representation of the flow of data to specify the program. Frequently used for reacting to discrete events or for processing streams of data. Examples of dataflow languages include:
Data-oriented languages provide powerful ways of searching and manipulating the relations that have been described as entity relationship tables which map one set of things into other sets. Examples of data-oriented languages include:
Data-structured languages are those where logic is structured in ways similar to their data. Such languages are generally well suited to reflection and introspection. There are three main types:
Assembly languages that statically link data inline with instructions can also be considered data-structured, in the most primitive way.
Decision tables can be used as an aid to clarifying the logic before writing a program in any language, but in the 1960s a number of languages were developed where the main logic is expressed directly in the form of a decision table, including:
Declarative languages describe a problem rather than defining a solution. Declarative programming stands in contrast to imperative programming via imperative programming languages, where serial orders (imperatives) are given to a computer. In addition to the examples given just below, all (pure) functional and logic-based programming languages are also declarative. In fact, "functional" and "logical" constitute the usual subcategories of the declarative category.
Source embeddable languages embed small pieces of executable code inside a piece of free-form text, often a web page.
Client-side embedded languages are limited by the abilities of the browser or intended client. They aim to provide dynamism to web pages without the need to recontact the server.
Server-side embedded languages are much more flexible, since almost any language can be built into a server. The aim of having fragments of server-side code embedded in a web page is to generate additional markup dynamically; the code itself disappears when the page is served, to be replaced by its output.
The above examples are particularly dedicated to this purpose. A large number of other languages, such as Erlang, Scala, Perl and Ruby can be adapted (for instance, by being made into Apache modules).
A wide variety of dynamic or scripting languages can be embedded in compiled executable code. Basically, object code for the language's interpreter needs to be linked into the executable. Source code fragments for the embedded language can then be passed to an evaluation function as strings. Application control languages can be implemented this way, if the source code is input by the user. Languages with small interpreters are preferred.
Languages developed primarily for the purpose of teaching and learning of programming.
An esoteric programming language is a programming language designed as a test of the boundaries of computer programming language design, as a proof of concept, or as a joke.
Extension programming languages are languages embedded into another program and used to harness its features in extension scripts.
Fourth-generation programming languages are high-level languages built around database systems. They are generally used in commercial environments.
Functional programming languages define programs and subroutines as mathematical functions. Many so-called functional languages are "impure", containing imperative features. Many functional languages are tied to mathematical calculation tools. Functional languages include:
In electronics, a Hardware description language or HDL is a specialized computer language used to describe the structure, design and operation of electronic circuits, and most commonly, digital logic circuits. The two most widely used and well-supported HDL varieties used in industry are Verilog and VHDL. Hardware description languages include:
Imperative programming languages may be multi-paradigm and appear in other classifications. Here is a list of programming languages that follow the imperative paradigm:
Interactive mode languages act as a kind of shell: expressions or statements can be entered one at a time, and the result of their evaluation is seen immediately. The interactive mode is also known as a REPL (read–eval–print loop).
Interpreted languages are programming languages in which programs may be executed from source code form, by an interpreter. Theoretically, any language can be compiled or interpreted, so the term *interpreted language* generally refers to languages that are commonly interpreted rather than compiled.
Iterative languages are built around or offering generators.
List-based languages are a type of data-structured language that are based upon the list data structure.
Little languages serve a specialized problem domain.
Logic-based languages specify a set of attributes that a solution must have, rather than a set of steps to obtain a solution. Examples:
Machine languages are directly executable by a computer's CPU. They are typically formulated as bit patterns, usually represented in octal or hexadecimal. Each bit pattern causes the circuits in the CPU to execute one of the fundamental operations of the hardware. The activation of specific electrical inputs (e.g., CPU package pins for microprocessors), and logical settings for CPU state values, control the processor's computation. Individual machine languages are specific to a family of processors; machine-language code for one family of processors cannot run directly on processors in another family unless the processors in question have additional hardware to support it (for example, DEC VAX processors included a PDP-11 compatibility mode). They are (essentially) always defined by the CPU developer, not by 3rd parties. The symbolic version, the processor's assembly language, is also defined by the developer, in most cases. Some commonly used machine code instruction sets are:
Macro languages transform one source code file into another. A "macro" is essentially a short piece of text that expands into a longer one (not too be confused with hygienic macros), possibly with parameter substitution. They are often used to preprocess source code. Preprocessors can also supply facilities like file inclusion.
Macro languages may be restricted to acting on specially labeled code regions (pre-fixed with a # in the case of the C preprocessor). Alternatively, they may not, but in this case it is still often undesirable to (for instance) expand a macro embedded in a string literal, so they still need a rudimentary awareness of syntax. That being the case, they are often still applicable to more than one language. Contrast with source-embeddable languages like PHP, which are fully featured.
Scripting languages such as Tcl and ECMAScript (ActionScript, ECMAScript for XML, JavaScript, JScript) have been embedded into applications. These are sometimes called "macro languages", although in a somewhat different sense to textual-substitution macros like m4.
Metaprogramming is writing of programs that write or manipulate other programs (or themselves) as their data or that do part of the work that is otherwise done at run time during compile time. In many cases, this allows programmers to get more done in the same amount of time as they would take to write all the code manually.
Multiparadigm languages support more than one programming paradigm. They allow a program to use more than one programming style. The goal is to allow programmers to use the best tool for a job, admitting that no one paradigm solves all problems in the easiest or most efficient way.
Class-based Object-oriented programming languages support objects defined by their class. Class definitions include member data. Message passing is a key concept (if not the key concept) in Object-oriented languages.
Polymorphic functions parameterized by the class of some of their arguments are typically called methods. In languages with single dispatch, classes typically also include method definitions. In languages with multiple dispatch, methods are defined by generic functions. There are exceptions where single dispatch methods are generic functions (e.g. Bigloo's object system).
Prototype-based languages are object-oriented languages where the distinction between classes and instances has been removed:
Off-side rule languages are those where blocks are formed, indicated, by their indentation.
Procedural programming languages are based on the concept of the unit and scope (the data viewing range of an executable code statement). A procedural program is composed of one or more units or modules, either user coded or provided in a code library; each module is composed of one or more procedures, also called a function, routine, subroutine, or method, depending on the language. Examples of procedural languages include:
Reflective languages let programs examine and possibly modify their high level structure at runtime. This is most common in high-level virtual machine programming languages like Smalltalk, and less common in lower-level programming languages like C. Languages and platforms supporting reflection:
Rule-based languages instantiate rules when activated by conditions in a set of data. Of all possible activations, some set is selected and the statements belonging to those rules execute. Rule-based languages include:[citation needed]
"Scripting language" has two apparently different, but in fact similar meanings. In a traditional sense, scripting languages are designed to automate frequently used tasks that usually involve calling or passing commands to external programs. Many complex application programs provide built-in languages that let users automate tasks. Those that are interpretive are often called scripting languages.
Recently, many applications have built-in traditional scripting languages, such as Perl or Visual Basic, but there are quite a few native scripting languages still in use. Many scripting languages are compiled to bytecode and then this (usually) platform-independent bytecode is run through a virtual machine (compare to Java virtual machine).
Stack-based languages are a type of data-structured language that are based upon the stack data structure.
Synchronous programming languages are optimized for programming reactive systems, systems that are often interrupted and must respond quickly. Many such systems are also called realtime systems, and are used often in embedded systems. Examples:
These languages assist with generating lexical analyzers and parsers for Context-free grammars.
Visual programming languages let users specify programs in a two-(or more)-dimensional way, instead of as one-dimensional text strings, via graphic layouts of various types.
Some dataflow programming languages are also visual languages.
Computer scientist Niklaus Wirth designed and implemented several influential languages.
These are languages based on or that operate on XML.
Here, a genealogy of programming languages is shown. Languages are categorized under the ancestor language with the strongest influence. Those ancestor languages are listed in alphabetical order. Of course, any such categorization has a large arbitrary element, since programming languages often incorporate major ideas from multiple sources.


Spyce is technology similar to PHP that can be used to embed Python code into webpages. Spyce is free software, distributed under a BSD-style licence, with some additional restrictions about documentation notices.[1]


Since Python uses indentation to determine the beginning and end of a block, Spyce includes several ways to embed Python code. Shown below are the three most common ways. Spyce supports ASP/JSP-style delimiters (<% and %>) as well as double braces ([[ and ]])
1. Python 'chunks' (multiple Python statements with traditional indentation):
or
2. Individual statements within delimiters (indentation not required):
or
3. Expressions evaluation
or
The techniques above can be freely mixed and embedded in any HTML document.
Any legal Python code can be embedded and any Python module can be imported, which makes it especially suited for writing very robust applications (using exception handling and unit testing single modules individually).
Some other features include custom tags (ala JSP), spyce lambdas and active handlers (reminiscent of ASP).
Spyce brings Python's standard library and the programming language itself to the web. The minimum requirement is a working Python installation (it ships with a standalone web server written in Python that can be used during development), although it can be used in conjunction with several web servers such as Apache and IIS in a variety of ways.
Configuration is done using Python modules that are imported by the web server during initialization, so all that is really required to get started with Spyce is basic knowledge of Python.
SciPy (pronounced "Sigh Pie"[2]) is an open source Python library used for scientific computing and technical computing.
SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.
SciPy builds on the NumPy array object and is part of the NumPy stack which includes tools like Matplotlib, pandas and SymPy. There is an expanding set of scientific computing libraries that are being added to the NumPy stack every day. This NumPy stack has similar users to other applications such as MATLAB, GNU Octave, and Scilab. The NumPy stack is also sometimes referred to as the SciPy stack.[3]
SciPy is also a family of conferences for users and developers of these tools: SciPy (in the United States), EuroSciPy (in Europe) and SciPy.in (in India).[4] Enthought originated the SciPy conference in the United States and continues to sponsor many of the international conferences as well as host the SciPy website.
The SciPy library is currently distributed under the BSD license, and its development is sponsored and supported by an open community of developers. It is also supported by Numfocus which is a community foundation for supporting reproducible and accessible science.


A typical Python Scientific Computing Environment includes many dedicated software tools.
The SciPy package of key algorithms and functions core to Python's scientific computing capabilities. Available sub-packages include:
The basic data structure used by SciPy is a multidimensional array provided by the NumPy module. NumPy provides some functions for linear algebra, Fourier transforms and random number generation, but not with the generality of the equivalent functions in SciPy. NumPy can also be used as an efficient multi-dimensional container of data with arbitrary data-types. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. Older versions of SciPy used Numeric as an array type, which is now deprecated in favor of the newer NumPy array code.[6]
In the 1990s, Python was extended to include an array type for numerical computing called Numeric (This package was eventually replaced by Travis Oliphant who wrote NumPy in 2006 as a blending of Numeric and Numarray which had been started in 2001). As of 2000, there was a growing number of extension modules and increasing interest in creating a complete environment for scientific and technical computing. In 2001, Travis Oliphant, Eric Jones, and Pearu Peterson merged code they had written and called the resulting package SciPy. The newly created package provided a standard collection of common numerical operations on top of the Numeric array data structure. Shortly thereafter, Fernando Pérez released IPython, an enhanced interactive shell widely used in the technical computing community, and John Hunter released the first version of Matplotlib, the 2D plotting library for technical computing. Since then the SciPy environment has continued to grow with more packages and tools for technical computing.[7][8][9]
GitHub is a web-based Git or version control repository and Internet hosting service. It offers all of the distributed version control and source code management (SCM) functionality of Git as well as adding its own features. It provides access control and several collaboration features such as bug tracking, feature requests, task management, and wikis for every project.[3]
GitHub offers both plans for private and free repositories on the same account[4] which are commonly used to host open-source software projects.[5] As of April 2017, GitHub reports having almost 20 million users and 57 million repositories,[6] making it the largest host of source code in the world.[7]
GitHub has a mascot called Octocat, a cat with five tentacles and a human-like face.[8][9]


Development of the GitHub platform began on 1 October 2007.[10][11] The site was launched in April 2008 by Tom Preston-Werner, Chris Wanstrath, and PJ Hyett after it had been made available for a few months prior as a beta release.[12]
Projects on GitHub can be accessed and manipulated using the standard Git command-line interface and all of the standard Git commands work with it. GitHub also allows registered and non-registered users to browse public repositories on the site. Multiple desktop clients and Git plugins have also been created by GitHub and other third parties that integrate with the platform.
The site provides social networking-like functions such as feeds, followers, wikis (using wiki software called Gollum) and a social network graph to display how developers work on their versions ("forks") of a repository and what fork (and branch within that fork) is newest.
A user must create an account in order to contribute content to the site, but public repositories can be browsed and downloaded by anyone. With a registered user account, users are able to discuss, manage, create repositories, submit contributions to others' repositories, and review changes to code.
The software that runs GitHub was written using Ruby on Rails and Erlang by GitHub, Inc. developers Chris Wanstrath,[13] PJ Hyett, and Tom Preston-Werner.
GitHub is mostly used for code.
In addition to source code, GitHub supports the following formats and features:
GitHub's Terms of Service do not require public software projects hosted on GitHub to meet the Open Source Definition. For that reason, it is essential for users and developers intending to use a piece of software found on GitHub to read the software license in the repository (usually found in a top-level file called "LICENSE", "LICENSE.txt", or similar) to determine if it meets their needs[citation needed]. The Terms of Service state, "By setting your repositories to be viewed publicly, you agree to allow others to view and fork your repositories."[18]
GitHub Enterprise is similar to GitHub's public service but is designed for use by large-scale enterprise software development teams where the enterprise wishes to host their repositories behind a corporate firewall.[19]
GitHub also operates other services: a pastebin-style site called Gist[12] that is for hosting code snippets (GitHub proper is for hosting larger projects), and a slide hosting service called Speaker Deck.
Tom Preston-Werner presented the then-new Gist feature at a punk rock Ruby conference in 2008.[20] Gist builds on the traditional simple concept of a pastebin by adding version control for code snippets, easy forking, and SSL encryption for private pastes. Because each "gist" has its own Git repository, multiple code snippets can be contained in a single paste and they can be pushed and pulled using Git. Further, forked code can be pushed back to the original author in the form of a patch, so gists (pastes) can become more like mini-projects.
GitHub launched a new program called the GitHub Student Developer Pack to give students free access to popular development tools and services. GitHub partnered with Bitnami, Crowdflower, DigitalOcean, DNSimple, HackHands, Namecheap, Orchestrate, Screenhero, SendGrid, Stripe, Travis CI and Unreal Engine to launch the program.[21]
On 24 February 2009, GitHub team members announced, in a talk at Yahoo! headquarters, that within the first year of being online, GitHub had accumulated over 46,000 public repositories, 17,000 of which were formed in the previous month alone. At that time, about 6,200 repositories had been forked at least once and 4,600 had been merged.
On 5 July 2009, GitHub announced that the site was now harnessed by over 100,000 users. On 27 July 2009, In another talk delivered at Yahoo!, Tom Preston-Werner announced that GitHub had grown to host 90,000 unique public repositories, 12,000 having been forked at least once, for a total of 135,000 repositories.[22]
On 25 July 2010, GitHub announced that it hosts 1 million repositories.[23] On 20 April 2011, GitHub announced that it is hosting 2 million repositories.[24]
On 2 June 2011, ReadWriteWeb reported that GitHub had surpassed SourceForge and Google Code in total number of commits for the period January to May 2011.[25]
On 9 July 2012, Peter Levine, general partner at GitHub's investor Andreessen Horowitz, stated that GitHub had been growing revenue at 300% annually since 2008 "profitably nearly the entire way".[26]
On 16 January 2013, GitHub announced it had passed the 3 million users mark and was then hosting more than 5 million repositories.[27] On 23 December 2013, GitHub announced it had reached 10 million repositories.[28]
In June 2015, GitHub opened an office in Japan that is its first office outside of the U.S.[29]
On 29 July 2015, GitHub announced it had raised $250 million in funding in a round led by Sequoia Capital. The round valued the company at approximately $2 billion.[30]
In 2016, GitHub was ranked #14 on the Forbes Cloud 100 list.[31]
On 3 December 2014, GitHub was blocked in Russia for a few days over user-posted suicide manuals.[32]
On 31 December 2014, GitHub was blocked in India (along with 31 other Websites) over pro-ISIS content posted by users.[33] On 10 January 2015, GitHub was unblocked. Again, on 12 Sep 2015, GitHub was blocked all over India. The site was unblocked soon after.
On 26 March 2015, GitHub fell victim to a massive distributed denial-of-service (DDOS) attack that lasted for more than 118 hours.[34] The attack, which appeared to originate from China, primarily targeted GitHub-hosted user content describing methods of circumventing Internet censorship.[35][36][37]
On 8 October 2016, GitHub access was blocked by the Turkish government to prevent email leakage of a hacked account belonging to the country's Energy Minister.[38]
In March 2014, GitHub programmer Julie Ann Horvath alleged that founder and CEO Tom Preston-Werner and his wife Theresa engaged in a pattern of harassment against her that led to her leaving the company.[39] In April 2014, GitHub released a statement denying Horvath's allegations.[40][41] However, following an internal investigation, GitHub confirmed the claims. GitHub's CEO Chris Wanstrath wrote on the company blog, "The investigation found Tom Preston-Werner in his capacity as GitHub’s CEO acted inappropriately, including confrontational conduct, disregard of workplace complaints, insensitivity to the impact of his spouse's presence in the workplace, and failure to enforce an agreement that his spouse should not work in the office."[42] Preston-Werner then resigned from the company.
GitHub's mascot, Octocat, is an anthropomorphized female cat with five octopus-like tentacles.[8][9] The character was created by graphic designer Simon Oxley as clip art to sell on iStock,[43] a website that enables designers to market royalty-free digital images.
GitHub became interested in Oxley's work after Twitter selected a bird that he designed for their own logo.[44] The illustration GitHub chose was a character that Oxley had named Octopuss.[43] Since GitHub wanted Octopuss for their logo (a use that the iStock license disallows), they negotiated with Oxley to buy exclusive rights to the image.[43]
GitHub renamed Octopuss to Octocat,[43] and trademarked the character along with the new name.[8] Later, GitHub hired illustrator Cameron McEfee to adapt Octocat for different purposes on the website and promotional materials; McEfee and various GitHub users have since made hundreds of variations of the character.[45]
GitHub, Inc. was originally known as Logical Awesome LLC.[46]
As of December 2012[update], GitHub, Inc. was a flat organization with no middle managers; in other words, "everyone is a manager" (self-management).[47] Employees can choose to work on projects that interest them (open allocation). However, salaries are set by the chief executive.[48][needs update]
In 2014, GitHub, Inc. introduced a layer of middle management.[49]
GitHub.com is a start-up business, which in its first years provided enough revenue to be funded solely by its three founders and start taking on employees.[50] In July 2012, four years after the company was founded, Andreessen Horowitz invested $100M in venture capital.[3] In July 2015 GitHub raised another $250M of venture capital in a series B round. Investors were Sequoia Capital, Andreessen Horowitz, Thrive Capital and other venture capital funds.[51] As of August 2016, GitHub was making $140M in Annual Recurring Revenue.[52]

A metasyntactic variable is a meaningless word used as a placeholder in computer science, intended to be substituted by some objects pertaining to the context where it is used. The word foo as used in IETF Requests for Comments is a good example.[1]
By mathematical analogy, a metasyntactic variable is a word that is a variable for other words, just as in algebra letters are used as variables for numbers.[1]
"A standard convention is that any file with 'foo' in its name is temporary and can be deleted on sight."[2] The names of these consecrated "metasyntactic variables" are also commonly used as actual identifiers (for variables, functions, etc.) in tutorial programming examples when their purpose is to emphasize syntax.[3]


The term metasyntactic variable is an amalgamation of three components:
Both the IETF RFCs and computer programming languages are rendered in plain text, making it necessary to distinguish metasyntactic variables by a naming convention, more or less obvious from context. If rich text formatting is available, e.g. as in the HTML produced from texinfo sources, then a typographical convention may be used, as done for the example in the GNU Fortran manual:[4]
Plain text example:
RFC 772 (cited in RFC 3092) contains for instance:
(The documentation for texinfo emphasizes the distinction between metavariables and mere variables used in a programming language being documented in some texinfo file as: "Use the @var command to indicate metasyntactic variables. A metasyntactic variable is something that stands for another piece of text. For example, you should use a metasyntactic variable in the documentation of a function to describe the arguments that are passed to that function. Do not use @var for the names of particular variables in programming languages. These are specific names from a program, so @code is correct for them."[5])
Another point reflected in the above example is the convention that a metavariable is to be uniformly substituted with the same instance in all its appearances in a given schema. This is in contrast with nonterminal symbols in formal grammars where the nonterminals on the right of a production can be substituted by different instances.[6]
A third example of the use of the "metasyntactic variables" foo and bar, this time as actual identifiers in a programming (interview) example is contrasting the following C++ function prototypes for their different argument passing mechanisms:[7]
Metasyntactic variables used in the United States include foobar, foo, bar, baz, qux, quux, quuz, corge, grault, garply, waldo, fred, plugh, xyzzy, and thud.[1][8] Wibble, wobble, wubble, and flob are used in the UK.[9] And there are a reported blep, blah, and boop from Australia.[10][11]
Due to English being the foundation-language, or lingua franca, of most computer programming languages these variables are also commonly seen even in programs and examples of programs written for other spoken-language audiences.
The typical names may depend however on the subculture that has developed around a given programming language. For example, spam, ham, and eggs are the principal metasyntactic variables used in the Python programming language.[12] This is a reference to the comedy sketch Spam by Monty Python, the eponym of the language.[13]
The R programming language often adds norf to the list.[14]
In French, the words are toto and its derivatives, replacing o by other vowels: tata, titi, tutu... It may come from Toto who is the main character in many French jokes.[15][better source needed]
In Japanese, the words hoge and piyo are commonly used, with other common words and variants being fuga, hogera, and hogehoge.[16] Note that -ra is a pluralizing ending in Japanese, and reduplication is also used for pluralizing. The origin of hoge as a metasyntactic variable is not known, but it is believed to date to the early 1980s.[16]
In Portuguese, the words fulano, sicrano and beltrano are commonly used to refer to people.[17]
In Spanish, the words fulano,[18] mengano[19] perengano,[20] and zutano[21] are commonly used, often followed by de tal ("of such"), mocking a lastname in Spanish form (e.g. Fulano de Tal).
In Turkish, the words falan, filan, hede, hödö are commonly used.
In computer science, a set is an abstract data type that can store certain values, without any particular order, and no repeated values. It is a computer implementation of the mathematical concept of a finite set. Unlike most other collection types, rather than retrieving a specific element from a set, one typically tests a value for membership in a set.
Some set data structures are designed for static or frozen sets that do not change after they are constructed. Static sets allow only query operations on their elements — such as checking whether a given value is in the set, or enumerating the values in some arbitrary order. Other variants, called dynamic or mutable sets, allow also the insertion and deletion of elements from the set.
An abstract data structure is a collection, or aggregate, of data. The data may be booleans, numbers, characters, or other data structures. If one considers the structure yielded by packaging [a] or indexing,[b] there are four basic data structures:[1][2]
In this view, the contents of a set are a bunch, and isolated data items are elementary bunches (elements). Whereas sets contain elements, bunches consist of elements.
Further structuring may be achieved by considering the multiplicity of elements (sets become multisets, bunches become hyperbunches)[3] or their homogeneity (a record is a set of fields, not necessarily all of the same type).


In type theory, sets are generally identified with their indicator function (characteristic function): accordingly, a set of values of type 



A


{\displaystyle A}

 may be denoted by 




2

A




{\displaystyle 2^{A}}

 or 





P


(
A
)


{\displaystyle {\mathcal {P}}(A)}

. (Subtypes and subsets may be modeled by refinement types, and quotient sets may be replaced by setoids.) The characteristic function 



F


{\displaystyle F}

 of a set 



S


{\displaystyle S}

 is defined as:
In theory, many other abstract data structures can be viewed as set structures with additional operations and/or additional axioms imposed on the standard operations. For example, an abstract heap can be viewed as a set structure with a min(S) operation that returns the element of smallest value.
One may define the operations of the algebra of sets:
Typical operations that may be provided by a static set structure S are:
Dynamic set structures typically add:
Some set structures may allow only some of these operations. The cost of each operation will depend on the implementation, and possibly also on the particular values stored in the set, and the order in which they are inserted.
There are many other operations that can (in principle) be defined in terms of the above, such as:
Other operations can be defined for sets with elements of a special type:
Sets can be implemented using various data structures, which provide different time and space trade-offs for various operations. Some implementations are designed to improve the efficiency of very specialized operations, such as nearest or union. Implementations described as "general use" typically strive to optimize the element_of, add, and delete operations. A simple implementation is to use a list, ignoring the order of the elements and taking care to avoid repeated values. This is simple but inefficient, as operations like set membership or element deletion are O(n), as they require scanning the entire list.[d] Sets are often instead implemented using more efficient data structures, particularly various flavors of trees, tries, or hash tables.
As sets can be interpreted as a kind of map (by the indicator function), sets are commonly implemented in the same way as (partial) maps (associative arrays) – in this case in which the value of each key-value pair has the unit type or a sentinel value (like 1) – namely, a self-balancing binary search tree for sorted sets (which has O(log n) for most operations), or a hash table for unsorted sets (which has O(1) average-case, but O(n) worst-case, for most operations). A sorted linear hash table[11] may be used to provide deterministically ordered sets.
Further, in languages that support maps but not sets, sets can be implemented in terms of maps. For example, a common programming idiom in Perl that converts an array to a hash whose values are the sentinel value 1, for use as a set, is:
Other popular methods include arrays. In particular a subset of the integers 1..n can be implemented efficiently as an n-bit bit array, which also support very efficient union and intersection operations. A Bloom map implements a set probabilistically, using a very compact representation but risking a small chance of false positives on queries.
The Boolean set operations can be implemented in terms of more elementary operations (pop, clear, and add), but specialized algorithms may yield lower asymptotic time bounds. If sets are implemented as sorted lists, for example, the naive algorithm for union(S,T) will take time proportional to the length m of S times the length n of T; whereas a variant of the list merging algorithm will do the job in time proportional to m+n. Moreover, there are specialized set data structures (such as the union-find data structure) that are optimized for one or more of these operations, at the expense of others.
One of the earliest languages to support sets was Pascal; many languages now include it, whether in the core language or in a standard library.
As noted in the previous section, in languages which do not directly support sets but do support associative arrays, sets can be emulated using associative arrays, by using the elements as keys, and using a dummy value as the values, which are ignored.
A generalization of the notion of a set is that of a multiset or bag, which is similar to a set but allows repeated ("equal") values (duplicates). This is used in two distinct senses: either equal values are considered identical, and are simply counted, or equal values are considered equivalent, and are stored as distinct items. For example, given a list of people (by name) and ages (in years), one could construct a multiset of ages, which simply counts the number of people of a given age. Alternatively, one can construct a multiset of people, where two people are considered equivalent if their ages are the same (but may be different people and have different names), in which case each pair (name, age) must be stored, and selecting on a given age gives all the people of a given age.
Formally, it is possible for objects in computer science to be considered "equal" under some equivalence relation but still distinct under another relation. Some types of multiset implementations will store distinct equal objects as separate items in the data structure; while others will collapse it down to one version (the first one encountered) and keep a positive integer count of the multiplicity of the element.
As with sets, multisets can naturally be implemented using hash table or trees, which yield different performance characteristics.
The set of all bags over type T is given by the expression bag T. If by multiset one considers equal items identical and simply counts them, then a multiset can be interpreted as a function from the input domain to the non-negative integers (natural numbers), generalizing the identification of a set with its indicator function. In some cases a multiset in this counting sense may be generalized to allow negative values, as in Python.
Where a multiset data structure is not available, a workaround is to use a regular set, but override the equality predicate of its items to always return "not equal" on distinct objects (however, such will still not be able to store multiple occurrences of the same object) or use an associative array mapping the values to their integer multiplicities (this will not be able to distinguish between equal elements at all).
Typical operations on bags:
In relational databases, a table can be a (mathematical) set or a multiset, depending on the presence on unicity constraints on some columns (which turns it into a candidate key).
SQL allows the selection of rows from a relational table: this operation will in general yield a multiset, unless the keyword DISTINCT is used to force the rows to be all different, or the selection includes the primary (or a candidate) key.
In ANSI SQL the MULTISET keyword can be used to transform a subquery into a collection expression:
is a general select that can be used as subquery expression of another more general query, while
transforms the subquery into a collection expression that can be used in another query, or in assignment to a column of appropriate collection type.
First or 1st is the ordinal form of the number one (#1).
First or 1st may also refer to:
NetBSD is a free and open source Unix-like operating system that descends from Berkeley Software Distribution (BSD), a Research Unix derivative developed at the University of California, Berkeley. It was the second open-source BSD descendant formally released after it forked from the 386BSD[2] branch of the BSD source-code[3] repository. It continues to be actively developed and is available for many platforms, including large-scale server systems, desktop systems, and handheld devices,[3] and is often used in embedded systems.[4][5]
The NetBSD project focuses on code clarity, careful design, and portability across many computer architectures. NetBSD's source code is openly available and permissively licensed.[6][7]


NetBSD was originally derived from the 4.4BSD release of the Berkeley Software Distribution from the Computer Systems Research Group of the University of California, Berkeley, via their Net/2 source code release and the 386BSD project.[3] The NetBSD project began as a result of frustration within the 386BSD developer community with the pace and direction of the operating system's development.[8] The four founders of the NetBSD project, Chris Demetriou, Theo de Raadt, Adam Glass, and Charles Hannum, felt that a more open development model would benefit the project: one centered on portable, clean, correct code. They aimed to produce a unified, multi-platform, production-quality, BSD-based operating system. The name "NetBSD" was suggested by de Raadt[citation needed], based on the importance and growth of networks such as the Internet at that time, and the distributed, collaborative nature of its development.
The NetBSD source code repository was established on 21 March 1993 and the first official release, NetBSD 0.8, was made in April, 1993.[9] This was derived from 386BSD 0.1 plus the version 0.2.2 unofficial patchkit, with several programs from the Net/2 release missing from 386BSD re-integrated, and various other improvements.[9] The first multi-platform release, NetBSD 1.0, was made in October 1994. Also in 1994, for disputed reasons, one of the founders, Theo de Raadt, was removed from the project. He later founded a new project, OpenBSD, from a forked version of NetBSD 1.0 near the end of 1995.[10] In 1998, NetBSD 1.3 introduced the pkgsrc packages collection.[11]
Until 2004, NetBSD 1.x releases were made at roughly annual intervals, with minor "patch" releases in between. From release 2.0 onwards, NetBSD uses semantic versioning, and each major NetBSD release corresponds to an incremented major version number, i.e. the major releases following 2.0 are 3.0, 4.0 and so on. The previous minor releases are now divided into two categories: x.y "stable" maintenance releases and x.y.z releases containing only security and critical fixes.[12]
As the project's motto ("Of course it runs NetBSD" ) suggests, NetBSD has been ported to a large number of 32- and 64-bit architectures. These range from VAX minicomputers to Pocket PC PDAs. As of 2009, NetBSD supports 57 hardware platforms (across 15 different processor architectures). The kernel and userland for these platforms are all built from a central unified source-code tree managed by CVS. Currently, unlike other kernels such as μClinux, the NetBSD kernel requires the presence of an MMU in any given target architecture.
NetBSD's portability is aided by the use of hardware abstraction layer interfaces for low-level hardware access such as bus input/output or DMA. Using this portability layer, device drivers can be split into "machine-independent" and "machine-dependent" components. This makes a single driver easily usable on several platforms by hiding hardware access details, and reduces the work to port it to a new system.[13]
This permits a particular device driver for a PCI card to work without modifications, whether it's in a PCI slot on an IA-32, Alpha, PowerPC, SPARC, or other architecture with a PCI bus. Also, a single driver for a specific device can operate via several different buses, like ISA, PCI, or PC card.
In comparison, Linux device driver code often must be reworked for each new architecture. As a consequence, in porting efforts by NetBSD and Linux developers, NetBSD has taken much less time to port to new hardware.[14][better source needed]
This platform independence aids the development of embedded systems, particularly since NetBSD 1.6, when the entire toolchain of compilers, assemblers, linkers, and other tools fully support cross-compiling.
In 2005, as a demonstration of NetBSD's portability and suitability for embedded applications, Technologic Systems, a vendor of embedded systems hardware, designed and demonstrated a NetBSD-powered kitchen toaster.[15]
Commercial ports to embedded platforms, including the AMD Geode LX800, Freescale PowerQUICC processors, Marvell Orion, AMCC 405 family of PowerPC processors, Intel XScale IOP and IXP series, were available from and supported by Wasabi Systems.
The NetBSD cross-compiling framework (also known as "build.sh"[16]) lets a developer build a complete NetBSD system for an architecture from a more powerful system of different architecture (cross-compiling), including on a different operating system (the framework supports most POSIX-compliant systems). Several embedded systems using NetBSD have required no additional software development other than toolchain and target rehost.[14]
NetBSD features pkgsrc (short for "package source"), a framework for building and managing third-party application software packages. The pkgsrc collection consists of more than 12000 packages as of October 2012[update].[17] Building and installing packages such as KDE, GNOME, the Apache HTTP Server or Perl is performed through the use of a system of makefiles. This can automatically fetch the source code, unpack, patch, configure, build and install the package such that it can be removed again later. An alternative to compiling from source is to use a precompiled binary package. In either case, any prerequisites/dependencies will be installed automatically by the package system, without need for manual intervention.
pkgsrc supports not only NetBSD, but also several other BSD variants like FreeBSD and Darwin/Mac OS X, and other Unix-like operating systems such as Linux, Solaris, IRIX, and others, as well as Interix. pkgsrc was previously adopted as the official package management system for DragonFly BSD.[18]
NetBSD has supported SMP since the NetBSD 2.0 release in 2004,[19] which was initially implemented using the giant lock approach. During the development cycle of the NetBSD 5 release, major work was done to improve SMP support; most of the kernel subsystems were modified to be MP safe and use the fine-grained locking approach. New synchronization primitives were implemented and scheduler activations was replaced with a 1:1 threading model in February 2007.[20] A scalable M2 thread scheduler was implemented, though the old 4.4BSD scheduler still remains the default but was modified to scale with SMP. Threaded software interrupts were implemented to improve synchronization. The virtual memory system, memory allocator and trap handling were made MP safe. The file system framework, including the VFS and major file systems were modified to be MP safe. Since April, 2008 the only subsystems running with a giant lock are the network protocols and most device drivers.
NetBSD provides various features in the security area.[21] The Kernel Authorization framework[22] (or Kauth) is a subsystem managing all authorization requests inside the kernel, and used as system-wide security policy. It allows external modules to plug-in the authorization process. NetBSD also incorporates exploit mitigation features,[23] ASLR, restricted mprotect() and Segvguard from the PaX project, and GCC Stack Smashing Protection (SSP, or also known as ProPolice, enabled by default since NetBSD 6.0) compiler extensions. Verified Executables (or Veriexec) is an in-kernel file integrity subsystem in NetBSD. It allows the user to set digital fingerprints (hashes) of files, and take a number of different actions if files do not match their fingerprints. For example, one can allow Perl to run only scripts that match their fingerprints.[24] The cryptographic device driver (CGD) allows using disks or partitions (including CDs and DVDs) for encrypted storage.[25]
The Xen virtual-machine monitor has been supported in NetBSD since release 3.0. The use of Xen requires a special pre-kernel boot environment that loads a Xen-specialized kernel as the "host OS" (Dom0). Any number of "guest OSes" (DomU) virtualized computers, with or without specific Xen/DomU support, can be run in parallel with the appropriate hardware resources.
The need for a third-party boot manager, such as GRUB, was eliminated with NetBSD 5's Xen-compatible boot manager.[26] NetBSD 6 as a Dom0 has been benchmarked comparably to Linux, with better performance than Linux in some tests.[27]
User-space virtualization such as VirtualBox and QEMU are also supported on NetBSD.
NetBSD 5.0 introduced the rump kernel, an architecture to run drivers in user-space by emulating kernel-space calls. This anykernel architecture allows adding support of NetBSD drivers to other kernel architectures, ranging from exokernels to monolithic kernels.[28]
NetBSD includes many enterprise features like iSCSI, a journaling filesystem, logical volume management and the ZFS filesystem.
The WAPBL journaling filesystem, an extension of the BSD FFS filesystem, was contributed by Wasabi Systems in 2008.[29]
The NetBSD Logical Volume Manager is based on a BSD reimplementation of a device-mapper driver and a port of the Linux Logical Volume Manager tools. It was mostly written during the Google Summer of Code 2008.[30]
The ZFS filesystem developed by Sun Microsystems was imported into the NetBSD base system in 2009. Currently, the NetBSD ZFS port is based on ZFS version 22.
The CHFS Flash memory filesystem was imported into NetBSD in November 2011. CHFS is a file system developed at the Department of Software Engineering, University of Szeged, Hungary, and is the first open source Flash-specific file system written for NetBSD.
At the source code level, NetBSD is very nearly entirely compliant with POSIX.1 (IEEE 1003.1-1990) standard and mostly compliant with POSIX.2 (IEEE 1003.2-1992).
NetBSD provides system call-level binary compatibility on the appropriate processor architectures with its previous releases, but also with several other UNIX-derived and UNIX-like operating systems, including illumos/Solaris, Linux, other BSD variants like FreeBSD, Apple's Darwin/OS X[citation needed] and SunOS 4. This allows NetBSD users to run many applications that are only distributed in binary form for other operating systems, usually with no significant loss of performance.[31]
A variety of "foreign" disk filesystem formats are also supported in NetBSD, including FAT, NTFS, Linux ext2fs, OS X UFS, RISC OS FileCore/ADFS, AmigaOS Fast File System, IRIX EFS and many more through FUSE.
Kernel-space scripting with the Lua programming language is a relatively new feature in NetBSD; it is available as of NetBSD 7.0.[32] The Lua language (i.e., its interpreter and standard libraries) was initially ported to the NetBSD kernel during Google Summer of Code 2010 and has undergone several improvements since then. There are two main differences between user and kernel space Lua: kernel Lua does not support floating-point numbers; as such, only Lua integers are available. It also does not have full support to user space libraries that rely on the operating system (e.g., io and os).
All of the NetBSD kernel and most of the core userland source code is released under the terms of the BSD License (two, three, and four-clause variants). This essentially allows everyone to use, modify, redistribute or sell it as they wish, as long as they do not remove the copyright notice and license text (the four-clause variants also include terms relating to publicity material). Thus, the development of products based on NetBSD is possible without having to make modifications to the source code public. In contrast, the GPL, which does not apply to NetBSD, stipulates that changes to source code of a product must be released to the product recipient when products derived from those changes are released.
On 20 June 2008, the NetBSD Foundation announced a transition to the two clause BSD license, citing concerns with UCB support of clause 3 and industry applicability of clause 4.[33]
NetBSD also includes the GNU development tools and other packages, which are covered by the GPL and other open source licenses. As with other BSD projects, NetBSD separates those in its base source tree to make it easier to remove code that is under more restrictive licenses.[34] As for packages, the installed software licenses may be controlled by modifying the list of allowed licenses in the pkgsrc configuration file (mk.conf).
The following table lists major NetBSD releases and their notable features in reverse chronological order. Minor and patch releases are not included.
The NetBSD "flag" logo, designed by Grant Bissett, was introduced in 2004 and is an abstraction of their older logo,[53] designed by Shawn Mueller in 1994. Mueller's version was based on the famous World War II photograph Raising the Flag on Iwo Jima, which some perceived as culturally insensitive and inappropriate for an international project.[54]
The NetBSD Foundation is the legal entity that owns the intellectual property and trademarks associated with NetBSD,[55] and on 22 January 2004, became a 501(c)3 tax-exempt non-profit organization. The members of the foundation are developers who have CVS commit access.[56] The NetBSD Foundation has a Board of Directors, elected by the voting of members for two years.[57]
NetBSD's clean design, high performance, scalability, and support for many architectures has led to its use in embedded devices and servers, especially in networking applications.[58]
A commercial real-time operating system, QNX, uses a network stack based on NetBSD code,[59][60] and provides various drivers ported from NetBSD.[58]
Dell Force10 uses NetBSD as the underlying operating system that powers FTOS (the Force10 Operating System), which is used in high scalability switch/routers.[61] Force10 also made a donation to the NetBSD Foundation in 2007 to help further research and the open development community.[62]
Wasabi Systems provides a commercial Wasabi Certified BSD product based on NetBSD with proprietary enterprise features and extensions, which are focused on embedded, server and storage applications.[63]
NetBSD was used in NASA's SAMS-II Project of measuring the microgravity environment on the International Space Station,[64][better source needed] and for investigations of TCP for use in satellite networks.[65]
In 2004, SUNET used NetBSD to set the Internet2 Land Speed Record. NetBSD was chosen "due to the scalability of the TCP code".[66]
NetBSD is also used in Apple's AirPort Extreme and Time Capsule products,[67][68] instead of their own OS X (most of whose Unix-level userland code is derived from FreeBSD code but some is derived from NetBSD code[69][70]).
The operating system of the T-Mobile Sidekick LX 2009 smartphone is based on NetBSD.[71]
The Minix operating system uses a mostly NetBSD userland as well as its pkgsrc packages infrastructure since version 3.2.[72]
Hosting for the project is provided primarily by Columbia University, and Western Washington University. Mirrors for the project are spread around the world and provided by volunteers and supporters of the project.
Use of chmod
Use of mount
fstab, mount options rw,noauto
fstab, mount options rw,auto

CLR: MIT/X11[4]
Mono compiler: dual GPLv3 and MIT/X11
Libraries: LGPLv2
C#[note 2] (pronounced as see sharp) is a multi-paradigm programming language encompassing strong typing, imperative, declarative, functional, generic, object-oriented (class-based), and component-oriented programming disciplines. It was developed by Microsoft within its .NET initiative and later approved as a standard by Ecma (ECMA-334) and ISO (ISO/IEC 23270:2006). C# is one of the programming languages designed for the Common Language Infrastructure.
C# is a general-purpose, object-oriented programming language.[12] Its development team is led by Anders Hejlsberg. The most recent version is C# 7.0 which was released in 2017 along with Visual Studio 2017.[13]


The ECMA standard lists these design goals for C#:[12]
During the development of the .NET Framework, the class libraries were originally written using a managed code compiler system called Simple Managed C (SMC).[14][15] In January 1999, Anders Hejlsberg formed a team to build a new language at the time called Cool, which stood for "C-like Object Oriented Language".[16] Microsoft had considered keeping the name "Cool" as the final name of the language, but chose not to do so for trademark reasons. By the time the .NET project was publicly announced at the July 2000 Professional Developers Conference, the language had been renamed C#, and the class libraries and ASP.NET runtime had been ported to C#.
C#'s principal designer and lead architect at Microsoft is Anders Hejlsberg, who was previously involved with the design of Turbo Pascal, Embarcadero Delphi (formerly CodeGear Delphi, Inprise Delphi and Borland Delphi), and Visual J++. In interviews and technical papers he has stated that flaws[citation needed] in most major programming languages (e.g. C++, Java, Delphi, and Smalltalk) drove the fundamentals of the Common Language Runtime (CLR), which, in turn, drove the design of the C# language itself.
James Gosling, who created the Java programming language in 1994, and Bill Joy, a co-founder of Sun Microsystems, the originator of Java, called C# an "imitation" of Java; Gosling further said that "[C# is] sort of Java with reliability, productivity and security deleted."[17][18] Klaus Kreft and Angelika Langer (authors of a C++ streams book) stated in a blog post that "Java and C# are almost identical programming languages. Boring repetition that lacks innovation,"[19] "Hardly anybody will claim that Java or C# are revolutionary programming languages that changed the way we write programs," and "C# borrowed a lot from Java - and vice versa. Now that C# supports boxing and unboxing, we'll have a very similar feature in Java."[20] In July 2000, Anders Hejlsberg said that C# is "not a Java clone" and is "much closer to C++" in its design.[21]
Since the release of C# 2.0 in November 2005, the C# and Java languages have evolved on increasingly divergent trajectories, becoming somewhat less similar. One of the first major departures came with the addition of generics to both languages, with vastly different implementations. C# makes use of reification to provide "first-class" generic objects that can be used like any other class, with code generation performed at class-load time.[22] Furthermore, C# has added several major features to accommodate functional-style programming, culminating in the LINQ extensions released with C# 3.0 and its supporting framework of lambda expressions, extension methods, and anonymous types.[23] These features enable C# programmers to use functional programming techniques, such as closures, when it is advantageous to their application. The LINQ extensions and the functional imports help developers reduce the amount of "boilerplate" code that is included in common tasks like querying a database, parsing an xml file, or searching through a data structure, shifting the emphasis onto the actual program logic to help improve readability and maintainability.[24]
C# used to have a mascot called Andy (named after Anders Hejlsberg). It was retired on January 29, 2004.[25]
C# was originally submitted to the ISO subcommittee JTC 1/SC 22 for review,[26] under ISO/IEC 23270:2003,[27] was withdrawn and was then approved under ISO/IEC 23270:2006.[28]
The name "C sharp" was inspired by musical notation where a sharp indicates that the written note should be made a semitone higher in pitch.[29] This is similar to the language name of C++, where "++" indicates that a variable should be incremented by 1. The sharp symbol also resembles a ligature of four "+" symbols (in a two-by-two grid), further implying that the language is an increment of C++.[30]
Due to technical limitations of display (standard fonts, browsers, etc.) and the fact that the sharp symbol (U+266F ♯ MUSIC SHARP SIGN (HTML &#9839;)) is not present on most keyboard layouts, the number sign (U+0023 # NUMBER SIGN (HTML &#35;)) was chosen to approximate the sharp symbol in the written name of the programming language.[31] This convention is reflected in the ECMA-334 C# Language Specification.[12] However, when it is practical to do so (for example, in advertising or in box art[32]), Microsoft uses the intended musical symbol.
The "sharp" suffix has been used by a number of other .NET languages that are variants of existing languages, including J# (a .NET language also designed by Microsoft that is derived from Java 1.1), A# (from Ada), and the functional programming language F#.[33] The original implementation of Eiffel for .NET was called Eiffel#,[34] a name retired since the full Eiffel language is now supported. The suffix has also been used for libraries, such as Gtk# (a .NET wrapper for GTK+ and other GNOME libraries) and Cocoa# (a wrapper for Cocoa).
.NET Framework 2.0 (Except LINQ/Query Extensions)[35]
.NET Framework 3.0 (Except LINQ/Query Extensions)[35]
.NET Framework 3.5
The core syntax of C# language is similar to that of other C-style languages such as C, C++ and Java. In particular:
Some notable features of C# that distinguish it from C, C++, and Java where noted, are:
By design, C# is the programming language that most directly reflects the underlying Common Language Infrastructure (CLI).[46] Most of its intrinsic types correspond to value-types implemented by the CLI framework. However, the language specification does not state the code generation requirements of the compiler: that is, it does not state that a C# compiler must target a Common Language Runtime, or generate Common Intermediate Language (CIL), or generate any other specific format. Theoretically, a C# compiler could generate machine code like traditional compilers of C++ or Fortran.
C# supports strongly typed implicit variable declarations with the keyword var, and implicitly typed arrays with the keyword new[] followed by a collection initializer.
C# supports a strict Boolean data type, bool. Statements that take conditions, such as while and if, require an expression of a type that implements the true operator, such as the Boolean type. While C++ also has a Boolean type, it can be freely converted to and from integers, and expressions such as if(a) require only that a is convertible to bool, allowing a to be an int, or a pointer. C# disallows this "integer meaning true or false" approach, on the grounds that forcing programmers to use expressions that return exactly bool can prevent certain types of programming mistakes such as if (a = b) (use of assignment = instead of equality ==, which while not an error in C or C++, will be caught by the compiler anyway).
C# is more type safe than C++. The only implicit conversions by default are those that are considered safe, such as widening of integers. This is enforced at compile-time, during JIT, and, in some cases, at runtime. No implicit conversions occur between Booleans and integers, nor between enumeration members and integers (except for literal 0, which can be implicitly converted to any enumerated type). Any user-defined conversion must be explicitly marked as explicit or implicit, unlike C++ copy constructors and conversion operators, which are both implicit by default.
C# has explicit support for covariance and contravariance in generic types, unlike C++ which has some degree of support for contravariance simply through the semantics of return types on virtual methods.
Enumeration members are placed in their own scope.
The C# language does not allow for global variables or functions. All methods and members must be declared within classes. Static members of public classes can substitute for global variables and functions.
Local variables cannot shadow variables of the enclosing block, unlike C and C++.
Meta programming via C# attributes is part of the language. Many of these attributes duplicate the functionality of GCC's and VisualC++'s platform-dependent preprocessor directives.
Like C++, and unlike Java, C# programmers must use the keyword virtual to allow methods to be overridden by subclasses.
Extension methods in C# allow programmers to use static methods as if they were methods from a class's method table, allowing programmers to add methods to an object that they feel should exist on that object and its derivatives.
The type dynamic allows for run-time method binding, allowing for JavaScript-like method calls and run-time object composition.
C# has support for strongly-typed function pointers via the keyword delegate. Like the Qt framework's pseudo-C++ signal and slot, C# has semantics specifically surrounding publish-subscribe style events, though C# uses delegates to do so.
C# offers Java-like synchronized method calls, via the attribute [MethodImpl(MethodImplOptions.Synchronized)], and has support for mutually-exclusive locks via the keyword lock.
C# provides properties as syntactic sugar for a common pattern in which a pair of methods, accessor (getter) and mutator (setter) encapsulate operations on a single attribute of a class. No redundant method signatures for the getter/setter implementations need be written, and the property may be accessed using attribute syntax rather than more verbose method calls.
A C# namespace provides the same level of code isolation as a Java package or a C++ namespace, with very similar rules and features to a package.
In C#, memory address pointers can only be used within blocks specifically marked as unsafe, and programs with unsafe code need appropriate permissions to run. Most object access is done through safe object references, which always either point to a "live" object or have the well-defined null value; it is impossible to obtain a reference to a "dead" object (one that has been garbage collected), or to a random block of memory. An unsafe pointer can point to an instance of a value-type, array, string, or a block of memory allocated on a stack. Code that is not marked as unsafe can still store and manipulate pointers through the System.IntPtr type, but it cannot dereference them.
Managed memory cannot be explicitly freed; instead, it is automatically garbage collected. Garbage collection addresses the problem of memory leaks by freeing the programmer of responsibility for releasing memory that is no longer needed.
Checked exceptions are not present in C# (in contrast to Java). This has been a conscious decision based on the issues of scalability and versionability.[47]
Unlike C++, C# does not support multiple inheritance, although a class can implement any number of interfaces. This was a design decision by the language's lead architect to avoid complication and simplify architectural requirements throughout CLI. When implementing multiple interfaces that contain a method with the same signature, C# allows implementing each method depending on which interface that method is being called through, or, like Java, allows implementing the method once, and have that be the one invocation on a call through any of the class's interfaces.
However, unlike Java, C# supports operator overloading. Only the most commonly overloaded operators in C++ may be overloaded in C#.
Though primarily an imperative language, C# 2.0 offered limited support for functional programming through first-class functions and closures in the form of anonymous delegates. C# 3.0 expanded support for functional programming with the introduction of a lightweight syntax for lambda expressions, extension methods (an affordance for modules), and a list comprehension syntax in the form of a "query comprehension" language.
C# has a unified type system. This unified type system is called Common Type System (CTS).[48]
A unified type system implies that all types, including primitives such as integers, are subclasses of the System.Object class. For example, every type inherits a ToString() method.
CTS separates data types into two categories:[48]
Instances of value types do not have referential identity nor referential comparison semantics - equality and inequality comparisons for value types compare the actual data values within the instances, unless the corresponding operators are overloaded. Value types are derived from System.ValueType, always have a default value, and can always be created and copied. Some other limitations on value types are that they cannot derive from each other (but can implement interfaces) and cannot have an explicit default (parameterless) constructor. Examples of value types are all primitive types, such as int (a signed 32-bit integer), float (a 32-bit IEEE floating-point number), char (a 16-bit Unicode code unit), and System.DateTime (identifies a specific point in time with nanosecond precision). Other examples are enum (enumerations) and struct (user defined structures).
In contrast, reference types have the notion of referential identity - each instance of a reference type is inherently distinct from every other instance, even if the data within both instances is the same. This is reflected in default equality and inequality comparisons for reference types, which test for referential rather than structural equality, unless the corresponding operators are overloaded (such as the case for System.String). In general, it is not always possible to create an instance of a reference type, nor to copy an existing instance, or perform a value comparison on two existing instances, though specific reference types can provide such services by exposing a public constructor or implementing a corresponding interface (such as ICloneable or IComparable). Examples of reference types are object (the ultimate base class for all other C# classes), System.String (a string of Unicode characters), and System.Array (a base class for all C# arrays).
Both type categories are extensible with user-defined types.
Boxing is the operation of converting a value-type object into a value of a corresponding reference type.[48] Boxing in C# is implicit.
Unboxing is the operation of converting a value of a reference type (previously boxed) into a value of a value type.[48] Unboxing in C# requires an explicit type cast. A boxed object of type T can only be unboxed to a T (or a nullable T).[49]
Example:
The C# specification details a minimum set of types and class libraries that the compiler expects to have available. In practice, C# is most often used with some implementation of the Common Language Infrastructure (CLI), which is standardized as ECMA-335 Common Language Infrastructure (CLI).
The following is a very simple C# program, a version of the classic "Hello world" example:
What will display on the program is:
Each line has a purpose:
The above line of code tells the compiler to use System as a candidate prefix for types used in the source code. In this case, when the compiler sees use of the Console type later in the source code, it tries to find a type named Console, first in the current assembly, followed by all referenced assemblies. In this case the compiler fails to find such a type, since the name of the type is actually System.Console. The compiler then attempts to find a type named System.Console by using the System prefix from the using statement, and this time it succeeds. The using statement allows the programmer to state all candidate prefixes to use during compilation instead of always using full type names.
Above is a class definition. Everything between the following pair of braces describes Program.
This declares the class member method where the program begins execution. The .NET runtime calls the Main method. (Note: Main may also be called from elsewhere, like any other method, e.g. from another method of Program.) The static keyword makes the method accessible without an instance of Program. Each console application's Main entry point must be declared static. Otherwise, the program would require an instance, but any instance would require a program. To avoid that irresolvable circular dependency, C# compilers processing console applications (like that above) report an error, if there is no static Main method. The void keyword declares that Main has no return value.
This line writes the output. Console is a static class in the System namespace. It provides an interface to the standard input, output, and error streams for console applications. The program calls the Console method WriteLine, which displays on the console a line with the argument, the string "Hello, world!".
A GUI example:
This example is similar to the previous example, except that it generates a dialog box that contains the message "Hello, World!" instead of writing it to the console.
In August 2001, Microsoft Corporation, Hewlett-Packard and Intel Corporation co-sponsored the submission of specifications for C# as well as the Common Language Infrastructure (CLI) to the standards organization Ecma International. In December 2001, ECMA released ECMA-334 C# Language Specification. C# became an ISO standard in 2003 (ISO/IEC 23270:2003 - Information technology — Programming languages — C#). ECMA had previously adopted equivalent specifications as the 2nd edition of C#, in December 2002.
In June 2005, ECMA approved edition 3 of the C# specification, and updated ECMA-334. Additions included partial classes, anonymous methods, nullable types, and generics (somewhat similar to C++ templates).
In July 2005, ECMA submitted to ISO/IEC JTC 1, via the latter's Fast-Track process, the standards and related TRs. This process usually takes 6–9 months.
The C# language definition and the CLI are standardized under ISO and Ecma standards that provide reasonable and non-discriminatory licensing protection from patent claims.
Microsoft has agreed not to sue open source developers for violating patents in non-profit projects for the part of the framework that is covered by the OSP.[50] Microsoft has also agreed not to enforce patents relating to Novell products against Novell's paying customers[51] with the exception of a list of products that do not explicitly mention C#, .NET or Novell's implementation of .NET (The Mono Project).[52] However, Novell maintains that Mono does not infringe any Microsoft patents.[53] Microsoft has also made a specific agreement not to enforce patent rights related to the Moonlight browser plugin, which depends on Mono, provided it is obtained through Novell.[54]
The reference C# compiler is Microsoft Visual C#, which is open-source.[55]
Microsoft is leading the development of a new open-source C# compiler and set of tools, previously codenamed "Roslyn". The compiler, which is entirely written in managed code (C#), has been opened up and functionality surfaced as APIs. It is thus enabling developers to create refactoring and diagnostics tools.
Other C# compilers exist, often including an implementation of the Common Language Infrastructure and the .NET class libraries up to .NET 2.0:
PyLadies is an international mentorship group which focuses on helping more women become active participants in the Python open-source community.[1][2][3] The mission of the group is to create a diverse Python community through outreach, education, conferences and social gatherings. PyLadies also provides funding for women to attend open source conferences. The organisation was created by seven women in Los Angeles[4] in April 2011, with the aim of increasing the participation of women in computing. PyLadies became a multi-chapter organization with the founding of the Washington (D.C.)[5] chapter in August 2011. The group currently has more than 40 chapters around the world.[6]


PyLadies has conducted outreach events for both beginners and experienced users.[7][8] PyLadies has conducted hackathons, ladies' nights and workshops for Python enthusiasts.[9]
PyLadies also runs a mailing list for discussing issues related to gendergap in tech-community and programming in Python.

CPython is the reference implementation of the Python programming language. Written in C, CPython is the default and most widely used implementation of the language.
CPython is an interpreter. It has a foreign function interface with several languages including C, in which one must explicitly write bindings in a language other than Python.


A significant limitation of CPython is the use of a global interpreter lock (GIL) on each CPython interpreter process, which effectively disables concurrent Python threads within one process.[1] Concurrency can only be achieved with separate CPython interpreter processes managed by a multitasking operating system. This complicates communication between concurrent Python processes, though the multiprocessing module mitigates this somewhat. Much discussion took place on whether to remove the GIL from CPython. A set of "free threading" patches to CPython was submitted by Greg Stein, which effectively replaced GIL with fine-grained locking. However the patches were rejected due to the execution overhead they introduced into single-process code.[2]
Supported platforms include:[3]
PEP 11[5] lists platforms which are not supported in CPython by Python Software Foundation. These platforms can still be supported by external ports. These ports include:
External ports not integrated to Python Software Foundation's official version of CPython, with links to its main development site, often include additional modules for platform-specific functionalities, like graphics and sound API for PSP and SMS and camera API for S60. These ports include:
Unladen Swallow was an optimization branch of CPython, intended to be fully compatible and significantly faster. It aimed to achieve its goals by supplementing CPython's custom virtual machine with a just-in-time compiler built using LLVM.
The project had stated a goal of a speed improvement by a factor of five over CPython;[36] this goal was not met.[37]
The project was sponsored by Google, and the project owners, Thomas Wouters, Jeffrey Yasskin, and Collin Winter, are full-time Google employees,[38] however most project contributors are not Google employees.[39] Unladen Swallow is hosted on Google Code.[40]
Like many things regarding the Python language, the name Unladen Swallow is a Monty Python reference, specifically to the joke about the airspeed velocity of unladen swallows in Monty Python and the Holy Grail.
Although it fell short of all published goals, Unladen Swallow did produce some code which got added to the main Python implementation, such as improvements to the cPickle module.[41]
In July 2010, some observers speculated on whether the project was dead or dying, since the 2009 Q4 milestone had not yet been released.[42] The traffic on Unladen's mailing list had decreased from 500 messages in January 2010, to fewer than 10 in September 2010.[43] It has also been reported that Unladen lost Google's funding.[44] In November 2010, one of the main developers announced that "Jeffrey and I have been pulled on to other projects of higher importance to Google".[45]
The 2009 Q4 development branch was created on January 26, 2010,[46] but no advertising was made on the website. Further, regarding the long-term plans, and as the project missed the Python 2.7 release, a Python Enhancement Proposal (PEP)[37] was accepted, which proposed a merge of Unladen Swallow into a special py3k-jit branch of Python's official repository. As of July 2010, this work was ongoing.[47] This merging would have taken some time, since Unladen Swallow was originally based on Python 2.6[48] with which Python 3 broke compatibility (see Python 3000 for more details). However, the PEP was subsequently withdrawn.
In early 2011, it became clear that the project was stopped.[49]
CPython is one of several "production-quality" Python implementations including: Jython, written in Java for the Java virtual machine (JVM), PyPy, written in RPython and translated into C, and IronPython, which is written in C# for the Common Language Infrastructure. There are also several experimental implementations.[53]
